---
header-includes: |
    - \let\vec\mathbf
    - \def\nullsp{\mathcal{N}}
    - \def\qed{\hfill\blacksquare}
    - \DeclareMathOperator\sgn{sgn}
    - \DeclareMathOperator\dim{dim}
    - \DeclareMathOperator\row{row}
    - \DeclareMathOperator\col{col}
    - \DeclareMathOperator\span{span}
    - \DeclareMathOperator\ker{ker}
    - \DeclareMathOperator\im{im}
    - \DeclareMathOperator\rank{rank}
    - \DeclareMathOperator\det{det}
    - \DeclareMathOperator\Rot{Rot}
---

# $âˆƒ! \vec{w} âˆˆ V : T(\vec{v}) = âŸ¨\vec{v}, \vec{w}âŸ©$

Let $\vec{u}â‚, â€¦, \vec{u}_m$ be an orthonormal basis of $V$.
$$ T : V â†’ â„ $$
\begin{align*}
\vec{w} &= \sum_{i = 1}^m T(\vec{u}_i) \vec{u}_i \\
    &= T(\vec{u}â‚) \vec{u}â‚ + â‹¯ + T(\vec{u}_m) \vec{u}_m
\end{align*}
Let $\vec{v} = aâ‚ \vec{u}â‚ + â‹¯ + a_m \vec{u}_m âˆˆ V$, then
$T(\vec{v}) = aâ‚T(\vec{u}â‚) + â‹¯ + a_m T(\vec{u}_m)$. Finally
\begin{align*}
âŸ¨\vec{v}, \vec{w}âŸ© &= aâ‚ âŸ¨\vec{u}â‚, T(\vec{u}â‚) \vec{u}â‚ âŸ© +
                        â‹¯ + a_m âŸ¨\vec{u}_m, T(\vec{u}_m) \vec{u}_m âŸ© \\
    &= aâ‚ T(\vec{u}â‚) + â‹¯ +  a_m T(\vec{u}_m) \\
    &= T(\vec{v})
\end{align*}

# $\row(A) \cap \mathcal{N}(A) = \{ \vec{0} \}$ where $A âˆˆ Râ¿$

$A âˆˆ â„^{mÃ—n}$ with rows $A = (\vec{a}â‚ â‹¯ \vec{a}_m)^T$.
Denote by $t_\vec{a}(\vec{x}) = âŸ¨\vec{a}, \vec{x}âŸ© = aâ‚xâ‚ + â‹¯ + a_n x_n$.
Then the corresponding map is
$$ T_A = \begin{pmatrix}
t_{\vec{a}â‚} \\
\vdots \\
t_{\vec{a}_m}
\end{pmatrix} $$

Let $\vec{x} âˆˆ \row(A) \cap \nullsp(A)$ then
$$ A \vec{x} = \begin{pmatrix}
âŸ¨\vec{a}â‚, \vec{x}âŸ© \\
\vdots \\
âŸ¨\vec{a}_m, \vec{x}âŸ© \\
\end{pmatrix} = \vec{0} $$
but since $\vec{x} âˆˆ \row(A)$
$$ \vec{x} = râ‚ \vec{a}â‚ + â‹¯ + r_m \vec{a}_m $$
and $âŸ¨\vec{a}_i, \vec{x}âŸ© = 0 â‡’ râ‚ âŸ¨\vec{a}â‚, \vec{x}âŸ© + â‹¯ + r_m âŸ¨\vec{a}_m, \vec{x}âŸ© = âŸ¨\vec{x}, \vec{x}âŸ© = 0$.
Therefore $\vec{x} = \vec{0}$.

# Linear Maps are Determined by Their Basis

$T : V â†’ W$, then $\vec{x} = râ‚ \vec{v}â‚ + â‹¯ + r_n \vec{v}_n$ which is unique.
$$ T(\vec{x}) = râ‚ T(\vec{v}â‚) + â‹¯ + r_n T(\vec{v}_n) $$
Likewise using a similar analysis we get the Rank-Nullity Theorem (7.4)
$$ \dim \ker(T) + \dim \im(T) = \dim V $$

## $V$ and $W$ over the same field are isomorphic $âŸº$ they have the same dimension

Choose basis $\vec{v}â‚, â€¦, \vec{v}_n$ of $V$, and $\vec{w}â‚, â€¦, \vec{w}_n$ of $W$.

Let $T: V â†’ W$ be the unique linear mapping $T(\vec{v}_i) = \vec{w}_i$ (guaranteed by above proposition).

So $\vec{w}â‚, â€¦, \vec{w}_n âˆˆ \im(T)$ and hence $T$ is surjective.

Also $â‡’ \dim \im(T) = \dim V$, and therefore $\dim \ker(T) = 0$. So $T$ is injective.

## Set of All Isomorphisms $GL(V)$

$GL(V)$ is the set of all isomorphisms $T: V â†’ V$.

If $V = Kâ¿$, then $GL(V) = GL(n, K)$.

# $\nullsp(A^TA) = \nullsp(A) \;\; âˆ€ A âˆˆ â„^{nÃ—n} â‡’ \rank(A^TA) = \rank(A)$

$$ (A^T A) b = \begin{pmatrix}
âŸ¨aâ‚, aâ‚âŸ© \hdots âŸ¨aâ‚, a_nâŸ© \\
\vdots \\
âŸ¨a_n, aâ‚âŸ© \hdots âŸ¨a_n, a_nâŸ©
\end{pmatrix} b = 0 $$
for all $i$
$$ âŸ¨a_i, râ‚ aâ‚ + â‹¯ + r_n a_nâŸ© = 0 $$
$$ â‡’ \vec{x} = (râ‚ â‹¯ r_n)^T âˆˆ \nullsp(A) $$

# $TS$ is an Isomorphism $âŸº S$ injective, $T$ surjective, and $\dim U = \dim W$

$U, V, W$ are finite-dimension vector spaces over a field $K$.
$$ S: U â†’ V $$
$$ T: V â†’ W $$
are linear.

## $TS$ is injective $âŸº S$ is injective and $\im(S) âˆ© \ker(T) = \{ \vec{0} \}$

$TS$ is injective so $\ker TS = \{ \vec{0} \}$. Restricting $T$, we see
$$ S : U â†’ V $$
$$ T : \im(S) â†’ W $$
that both must be injective, so that $\ker S = \{ \vec{0} \}$ and
$$ \ker(T) âˆ© \im(S) = \{ \vec{0} \} $$

## $TS$ is surjective $âŸº T$ is surjective and $V = \im(S) + \ker(T)$

For $TS: U â†’ V â†’ W$ to be surjective, obviously $T$ is surjective.

Now since $TS$ is surjective, then $T(\im(S)) = W$, and $V = \im(S) + \ker(T)$.

## $TS$ is an isomorphism $âŸº S$ is injective, $T$ is surjective, and $\dim(U) = \dim(W)$

$S$ is injective and finite dimension, means it is also surjective and an isomorphism.
Likewise for $T$ which is surjective.
We can see this by letting $ğœ™ : A â†’ B$ then
$$ \dim A = \dim\ker(ğœ™) + \dim \im(ğœ™) $$

$$ U \xrightarrow{S} V \xrightarrow{T} W $$
are isomorphisms. So $\ker(S) = \{ \vec{0}_U \}, \ker(T) = \{ \vec{0}_V \}$.
So $\dim \ker(S) = \dim \ker(T) = 0$, and $\dim \im(S) = \dim(V), \dim \im(T) = \dim(W)$.

Using the Rank-Nullity theorem, we see $\dim(U) = \dim(V)$ for $S$ and $\dim(V) = \dim(W)$ for $T$.

# Isometries and Orthogonal Linear Maps

\def \va {\vec{a}}
\def \vb {\vec{b}}
\def \vc {\vec{c}}
\def \vx {\vec{x}}
\def \vy {\vec{y}}
\def \ve {\vec{e}}
\def \vq {\vec{q}}
\def \vu {\vec{u}}
\def \vv {\vec{v}}
\def \vw {\vec{w}}
\def \vzero {\vec{0}}

Let $S : V â†’ V$ be a map then $âˆ€ \va, \vb âˆˆ V$, we have

* *Isometry*: $d(\va, \vb) = |\va - \vb| = d(S(\va), S(\vb))$ and $S$ is a linear map.
* *Orthogonal*: $âŸ¨S(\va), S(\vb)âŸ© = âŸ¨\va, \vbâŸ©$

## Orthogonal Maps are Linear

We have to show $S(\va + \vb) = S(\va) + S(\vb)$ which is equivalent to
$|S(\va + \vb) - S(\va) - S(\vb)|Â² = 0$.
Simply expand this out, and use the fact $âŸ¨S(\vx), S(\vy)âŸ© = âŸ¨\vx, \vyâŸ©$.

## Orthogonal Maps are Isometries

\begin{align*}
|S(\vb) - S(\va)|Â² &= |S(\vb - \va)|Â² \\
    &= âŸ¨S(\vb - \va), S(\va - \vb)âŸ© \\
    &= âŸ¨\vb - \va, \vb - \vaâŸ© \\
    &= |\vb - \va|Â²
\end{align*}

## Isometries are Orthogonal Maps

We have $|\vx - \vy|Â² = |S(\vx) - S(\vy)|Â²$ and so $|\vx|Â² = |S(\vx)|Â²$
\begin{align*}
|\va - \vb|Â² &= (âŸ¨\va, \vaâŸ© + âŸ¨\vb, \vbâŸ©) - 2âŸ¨\va, \vbâŸ© \\
    &= (âŸ¨S(\va), S(\va)âŸ© + âŸ¨S(\vb), S(\vb)âŸ©) - 2(\va, \vbâŸ©
\end{align*}
but $âŸ¨\va - \vb, \va - \vbâŸ© = âŸ¨S(\va) - S(\vb), S(\va) - S(\vb)âŸ©$
so we get the result
$$ 2âŸ¨\va, \vbâŸ© = 2âŸ¨S(\va), S(\vb)âŸ© $$

# Orthogonal Linear Maps on $â„â¿$

## Orthogonal maps are associated with a unique orthogonal matrix

Let $T : â„â¿ â†’ â„â¿$ be an isometry. Since $T$ is linear, it is determined by its
basis and there is a unique $Q âˆˆ â„^{nÃ—n}$ associated with $T$.

$T$ is orthogonal so $âŸ¨T_Q(\vx), T_Q(\vy)âŸ© = âŸ¨\vx, \vyâŸ©$.
But $âŸ¨T_Q(\vx), T_Q(\vy)âŸ© = âŸ¨Q\vx, Q\vyâŸ© = (Q\vx)^T (Q\vy)$
$â‡’ (Q\vx)^T (Q\vy) = \vx^T \vy$, thus
$$ \vx^T (Q^T Q) \vx = \vx^T \vy $$
Setting $Q\ve_i = \vq_i â‡’ \vq_i^T \vq_j = \ve_i^T \ve_j$ and so
$Q^T Q = I$.

Working backwards, we see the orthogonal matrix defines an isometry.

This means $O(n, â„)$ the orthogonal group is also the group of isometries of $â„â¿$.

# Projections

$$ P_\va(\vx) = \frac{âŸ¨\va, \vxâŸ©}{âŸ¨\va, \vaâŸ©} \va $$
is the *projection* onto $â„\va$, and is linear.

When $\va = \vu$ is a unit vector then we see that
$$ P_\vu(\vx) = \vu^T x \vu $$
Multiplying this out
\begin{align*}
P_\vu\begin{pmatrix} xâ‚ \\ xâ‚‚ \end{pmatrix} &=
    (uâ‚ xâ‚ + uâ‚‚ xâ‚‚) \begin{pmatrix} uâ‚ \\ uâ‚‚ \end{pmatrix} \\
    &= \begin{pmatrix}
        uâ‚Â² & uâ‚uâ‚‚ \\
        uâ‚uâ‚‚ & uâ‚‚Â²
    \end{pmatrix} \begin{pmatrix} xâ‚ \\ xâ‚‚ \end{pmatrix}
\end{align*}
This projection matrix of $P_\vu$ is symmetric. $\im(P_\vu) = â„\vu$,
while $\ker(P_\vu)$ is the line orthogonal to $â„\vu$.

A projection has the property that $P\vu âˆ˜ P_\vu = P_\vu$.
$$ P_\vu âˆ˜ P_\vu(\vv) = âŸ¨\vu, âŸ¨\vu, \vvâŸ©\vuâŸ© \vu = âŸ¨\vu, \vvâŸ©\vu = P_\vu(\vv) $$

1. $P_W$ is linear
2. $P_W(\vw) = \vw$ if $\vw âˆˆ W$
3. $P_W(W^\perp) = \{ \vzero \}$
4. $P_W + P_{W^\perp} = I$

# Reflections

Let $\vu âˆˆ â„“^\perp$ and decompose $\vv = P_\vu(\vv) + \vc$ with $c âˆˆ â„“$.
\begin{align*}
H(\vv) &= -P_\vu(\vv) + \vc \\
    &= -P_\vu(\vv) + (\vv - P_\vu(\vv)) \\
    &= \vv - 2P_\vu(\vv)
\end{align*}
Since $P_\vu$ is a linear map, so $H$ is a linear map.

## Reflection Matrix

$$ H(\vv) = \vv - 2P\vv $$
$$ Q = (I - 2P) = \begin{pmatrix}
uâ‚‚Â² - uâ‚Â² & -2uâ‚uâ‚‚ \\
-2uâ‚uâ‚‚ & uâ‚Â² - uâ‚‚Â²
\end{pmatrix} $$
using the identity $uâ‚Â² + uâ‚‚Â² = 1$

We can see this has the form $Q = \begin{pmatrix} a & b \\ b & -a \end{pmatrix}$ where
$aÂ² + bÂ² = 1$ and is a symmetric orthogonal matrix.

$$ QÂ² = (I - 2P)Â² = I - 4P + 4PÂ² = I $$
since $PÂ² = P$. Since $Q$ is symmetric, $Q = Q^T$ is therefore orthogonal.

# $O(2, â„)$-dichotomy

$$ \det: O(2, â„) â†’ â„^Ã— $$
$$ Q^T Q = I â‡’ \det(Q) = Â±1 $$

$$ Q^T Q = \begin{pmatrix}
aÂ² + bÂ²  &  ac + bd \\
ac + bd  &  cÂ² + dÂ²
\end{pmatrix} = I $$

```python
sage: var("a b c d")
(a, b, c, d)
sage: Q = matrix([[a, b], [c, d]])
sage: (Q.transpose() * Q).determinant().expand()
b^2*c^2 - 2*a*b*c*d + a^2*d^2
sage: 
sage: c = -b*d/a
sage: d2 = a^2 + b^2 - c^2
sage: d2 = d2.expand()
sage: d2
a^2 + b^2 - b^2*d^2/a^2
sage: e = d^2 - d2 == 0
sage: e
-a^2 - b^2 + b^2*d^2/a^2 + d^2 == 0
sage: e -= b^2*d^2/a^2 + d^2
sage: e
-a^2 - b^2 == -b^2*d^2/a^2 - d^2
sage: e /= (-b^2/a^2 - 1)
sage: e.simplify_full()
a^2 == d^2
```
Either $a = d$ or $a = -d$.

## $\ker(\det) = \Rot(2)$

Let $a = d â‡’ b = -c$ and $Qâ‚ = \begin{pmatrix}
a & -b \\
b & a \end{pmatrix}$.

Then $\det(Qâ‚) = aÂ² + bÂ² = 1$.
But this is the form of the rotation matrices
$R_Î¸ = \begin{pmatrix}
\cos Î¸ & -\sin Î¸ \\
\sin Î¸ & \cos Î¸
\end{pmatrix}$.

## $O(2, â„) - \Rot(2)$ are Reflections

Let $a = -d â‡’ b = c$ and $Qâ‚‚ = \begin{pmatrix}
a & b \\
b & -a \end{pmatrix}$.
Rewriting it
$$ Qâ‚‚ = \begin{pmatrix}
1 & 0 \\
0 & -1 \end{pmatrix} \begin{pmatrix}
a & -b \\
b & a \end{pmatrix} = \begin{pmatrix}
a & c \\
c & -a \end{pmatrix} $$
So $Qâ‚‚$ is a reflection and a rotation (coset of $Qâ‚$).

$\vv = (-c \;\; a + 1)^T, \vw = (a + 1 \;\; c)$ are orthogonal,
and $Q\vv = -\vv$, $Q\vw = \vw$. Normalize them to get an orthonormal basis.
