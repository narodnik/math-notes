---
header-includes: |
    - \let\vec\mathbf
    - \let\M\parenMatrixstack
    - \let\;\quad
    - \def\nullsp{\mathcal{N}}
    - \def\qed{\hfill\blacksquare}
    - \DeclareMathOperator\sgn{sgn}
    - \DeclareMathOperator\dim{dim}
    - \DeclareMathOperator\row{row}
    - \DeclareMathOperator\col{col}
    - \DeclareMathOperator\span{span}
    - \DeclareMathOperator\ker{ker}
    - \DeclareMathOperator\im{im}
    - \DeclareMathOperator\rank{rank}
    - \DeclareMathOperator\det{det}
    - \DeclareMathOperator\Rot{Rot}
---

# $âˆƒ! \vec{w} âˆˆ V : T(\vec{v}) = âŸ¨\vec{v}, \vec{w}âŸ©$

Let $\vec{u}â‚, â€¦, \vec{u}_m$ be an orthonormal basis of $V$.
$$ T : V â†’ â„ $$
\begin{align*}
\vec{w} &= \sum_{i = 1}^m T(\vec{u}_i) \vec{u}_i \\
    &= T(\vec{u}â‚) \vec{u}â‚ + â‹¯ + T(\vec{u}_m) \vec{u}_m
\end{align*}
Let $\vec{v} = aâ‚ \vec{u}â‚ + â‹¯ + a_m \vec{u}_m âˆˆ V$, then
$T(\vec{v}) = aâ‚T(\vec{u}â‚) + â‹¯ + a_m T(\vec{u}_m)$. Finally
\begin{align*}
âŸ¨\vec{v}, \vec{w}âŸ© &= aâ‚ âŸ¨\vec{u}â‚, T(\vec{u}â‚) \vec{u}â‚ âŸ© +
                        â‹¯ + a_m âŸ¨\vec{u}_m, T(\vec{u}_m) \vec{u}_m âŸ© \\
    &= aâ‚ T(\vec{u}â‚) + â‹¯ +  a_m T(\vec{u}_m) \\
    &= T(\vec{v})
\end{align*}

# $\row(A) \cap \mathcal{N}(A) = \{ \vec{0} \}$ where $A âˆˆ Râ¿$

$A âˆˆ â„^{mÃ—n}$ with rows $A = (\vec{a}â‚ â‹¯ \vec{a}_m)^T$.
Denote by $t_\vec{a}(\vec{x}) = âŸ¨\vec{a}, \vec{x}âŸ© = aâ‚xâ‚ + â‹¯ + a_n x_n$.
Then the corresponding map is
$$ T_A = \begin{pmatrix}
t_{\vec{a}â‚} \\
\vdots \\
t_{\vec{a}_m}
\end{pmatrix} $$

Let $\vec{x} âˆˆ \row(A) \cap \nullsp(A)$ then
$$ A \vec{x} = \begin{pmatrix}
âŸ¨\vec{a}â‚, \vec{x}âŸ© \\
\vdots \\
âŸ¨\vec{a}_m, \vec{x}âŸ© \\
\end{pmatrix} = \vec{0} $$
but since $\vec{x} âˆˆ \row(A)$
$$ \vec{x} = râ‚ \vec{a}â‚ + â‹¯ + r_m \vec{a}_m $$
and $âŸ¨\vec{a}_i, \vec{x}âŸ© = 0 â‡’ râ‚ âŸ¨\vec{a}â‚, \vec{x}âŸ© + â‹¯ + r_m âŸ¨\vec{a}_m, \vec{x}âŸ© = âŸ¨\vec{x}, \vec{x}âŸ© = 0$.
Therefore $\vec{x} = \vec{0}$.

# Linear Maps are Determined by Their Basis

$T : V â†’ W$, then $\vec{x} = râ‚ \vec{v}â‚ + â‹¯ + r_n \vec{v}_n$ which is unique.
$$ T(\vec{x}) = râ‚ T(\vec{v}â‚) + â‹¯ + r_n T(\vec{v}_n) $$
Likewise using a similar analysis we get the Rank-Nullity Theorem (7.4)
$$ \dim \ker(T) + \dim \im(T) = \dim V $$

## $V$ and $W$ over the same field are isomorphic $âŸº$ they have the same dimension

Choose basis $\vec{v}â‚, â€¦, \vec{v}_n$ of $V$, and $\vec{w}â‚, â€¦, \vec{w}_n$ of $W$.

Let $T: V â†’ W$ be the unique linear mapping $T(\vec{v}_i) = \vec{w}_i$ (guaranteed by above proposition).

So $\vec{w}â‚, â€¦, \vec{w}_n âˆˆ \im(T)$ and hence $T$ is surjective.

Also $â‡’ \dim \im(T) = \dim V$, and therefore $\dim \ker(T) = 0$. So $T$ is injective.

## Set of All Isomorphisms $GL(V)$

$GL(V)$ is the set of all isomorphisms $T: V â†’ V$.

If $V = Kâ¿$, then $GL(V) = GL(n, K)$.

# $\nullsp(A^TA) = \nullsp(A) \;\; âˆ€ A âˆˆ â„^{nÃ—n} â‡’ \rank(A^TA) = \rank(A)$

$$ (A^T A) b = \begin{pmatrix}
âŸ¨aâ‚, aâ‚âŸ© \hdots âŸ¨aâ‚, a_nâŸ© \\
\vdots \\
âŸ¨a_n, aâ‚âŸ© \hdots âŸ¨a_n, a_nâŸ©
\end{pmatrix} b = 0 $$
for all $i$
$$ âŸ¨a_i, râ‚ aâ‚ + â‹¯ + r_n a_nâŸ© = 0 $$
$$ â‡’ \vec{x} = (râ‚ â‹¯ r_n)^T âˆˆ \nullsp(A) $$

# $TS$ is an Isomorphism $âŸº S$ injective, $T$ surjective, and $\dim U = \dim W$

$U, V, W$ are finite-dimension vector spaces over a field $K$.
$$ S: U â†’ V $$
$$ T: V â†’ W $$
are linear.

## $TS$ is injective $âŸº S$ is injective and $\im(S) âˆ© \ker(T) = \{ \vec{0} \}$

$TS$ is injective so $\ker TS = \{ \vec{0} \}$. Restricting $T$, we see
$$ S : U â†’ V $$
$$ T : \im(S) â†’ W $$
that both must be injective, so that $\ker S = \{ \vec{0} \}$ and
$$ \ker(T) âˆ© \im(S) = \{ \vec{0} \} $$

## $TS$ is surjective $âŸº T$ is surjective and $V = \im(S) + \ker(T)$

For $TS: U â†’ V â†’ W$ to be surjective, obviously $T$ is surjective.

Now since $TS$ is surjective, then $T(\im(S)) = W$, and $V = \im(S) + \ker(T)$.

## $TS$ is an isomorphism $âŸº S$ is injective, $T$ is surjective, and $\dim(U) = \dim(W)$

$S$ is injective and finite dimension, means it is also surjective and an isomorphism.
Likewise for $T$ which is surjective.
We can see this by letting $ğœ™ : A â†’ B$ then
$$ \dim A = \dim\ker(ğœ™) + \dim \im(ğœ™) $$

$$ U \xrightarrow{S} V \xrightarrow{T} W $$
are isomorphisms. So $\ker(S) = \{ \vec{0}_U \}, \ker(T) = \{ \vec{0}_V \}$.
So $\dim \ker(S) = \dim \ker(T) = 0$, and $\dim \im(S) = \dim(V), \dim \im(T) = \dim(W)$.

Using the Rank-Nullity theorem, we see $\dim(U) = \dim(V)$ for $S$ and $\dim(V) = \dim(W)$ for $T$.

# Isometries and Orthogonal Linear Maps

\def \va {\vec{a}}
\def \vb {\vec{b}}
\def \vc {\vec{c}}
\def \vx {\vec{x}}
\def \vy {\vec{y}}
\def \ve {\vec{e}}
\def \vq {\vec{q}}
\def \vu {\vec{u}}
\def \vv {\vec{v}}
\def \vw {\vec{w}}
\def \vzero {\vec{0}}

Let $S : V â†’ V$ be a map then $âˆ€ \va, \vb âˆˆ V$, we have

* *Isometry*: $d(\va, \vb) = |\va - \vb| = d(S(\va), S(\vb))$ and $S$ is a linear map.
* *Orthogonal*: $âŸ¨S(\va), S(\vb)âŸ© = âŸ¨\va, \vbâŸ©$

## Orthogonal Maps are Linear

We have to show $S(\va + \vb) = S(\va) + S(\vb)$ which is equivalent to
$|S(\va + \vb) - S(\va) - S(\vb)|Â² = 0$.
Simply expand this out, and use the fact $âŸ¨S(\vx), S(\vy)âŸ© = âŸ¨\vx, \vyâŸ©$.

## Orthogonal Maps are Isometries

\begin{align*}
|S(\vb) - S(\va)|Â² &= |S(\vb - \va)|Â² \\
    &= âŸ¨S(\vb - \va), S(\va - \vb)âŸ© \\
    &= âŸ¨\vb - \va, \vb - \vaâŸ© \\
    &= |\vb - \va|Â²
\end{align*}

## Isometries are Orthogonal Maps

We have $|\vx - \vy|Â² = |S(\vx) - S(\vy)|Â²$ and so $|\vx|Â² = |S(\vx)|Â²$
\begin{align*}
|\va - \vb|Â² &= (âŸ¨\va, \vaâŸ© + âŸ¨\vb, \vbâŸ©) - 2âŸ¨\va, \vbâŸ© \\
    &= (âŸ¨S(\va), S(\va)âŸ© + âŸ¨S(\vb), S(\vb)âŸ©) - 2(\va, \vbâŸ©
\end{align*}
but $âŸ¨\va - \vb, \va - \vbâŸ© = âŸ¨S(\va) - S(\vb), S(\va) - S(\vb)âŸ©$
so we get the result
$$ 2âŸ¨\va, \vbâŸ© = 2âŸ¨S(\va), S(\vb)âŸ© $$

# Orthogonal Linear Maps on $â„â¿$

## Orthogonal maps are associated with a unique orthogonal matrix

Let $T : â„â¿ â†’ â„â¿$ be an isometry. Since $T$ is linear, it is determined by its
basis and there is a unique $Q âˆˆ â„^{nÃ—n}$ associated with $T$.

$T$ is orthogonal so $âŸ¨T_Q(\vx), T_Q(\vy)âŸ© = âŸ¨\vx, \vyâŸ©$.
But $âŸ¨T_Q(\vx), T_Q(\vy)âŸ© = âŸ¨Q\vx, Q\vyâŸ© = (Q\vx)^T (Q\vy)$
$â‡’ (Q\vx)^T (Q\vy) = \vx^T \vy$, thus
$$ \vx^T (Q^T Q) \vx = \vx^T \vy $$
Setting $Q\ve_i = \vq_i â‡’ \vq_i^T \vq_j = \ve_i^T \ve_j$ and so
$Q^T Q = I$.

Working backwards, we see the orthogonal matrix defines an isometry.

This means $O(n, â„)$ the orthogonal group is also the group of isometries of $â„â¿$.

# Projections

$$ P_\va(\vx) = \frac{âŸ¨\va, \vxâŸ©}{âŸ¨\va, \vaâŸ©} \va $$
is the *projection* onto $â„\va$, and is linear.

When $\va = \vu$ is a unit vector then we see that
$$ P_\vu(\vx) = \vu^T x \vu $$
Multiplying this out
\begin{align*}
P_\vu\begin{pmatrix} xâ‚ \\ xâ‚‚ \end{pmatrix} &=
    (uâ‚ xâ‚ + uâ‚‚ xâ‚‚) \begin{pmatrix} uâ‚ \\ uâ‚‚ \end{pmatrix} \\
    &= \begin{pmatrix}
        uâ‚Â² & uâ‚uâ‚‚ \\
        uâ‚uâ‚‚ & uâ‚‚Â²
    \end{pmatrix} \begin{pmatrix} xâ‚ \\ xâ‚‚ \end{pmatrix}
\end{align*}
This projection matrix of $P_\vu$ is symmetric. $\im(P_\vu) = â„\vu$,
while $\ker(P_\vu)$ is the line orthogonal to $â„\vu$.

A projection has the property that $P\vu âˆ˜ P_\vu = P_\vu$.
$$ P_\vu âˆ˜ P_\vu(\vv) = âŸ¨\vu, âŸ¨\vu, \vvâŸ©\vuâŸ© \vu = âŸ¨\vu, \vvâŸ©\vu = P_\vu(\vv) $$

1. $P_W$ is linear
2. $P_W(\vw) = \vw$ if $\vw âˆˆ W$
3. $P_W(W^\perp) = \{ \vzero \}$
4. $P_W + P_{W^\perp} = I$

# Reflections

Let $\vu âˆˆ â„“^\perp$ and decompose $\vv = P_\vu(\vv) + \vc$ with $c âˆˆ â„“$.
\begin{align*}
H(\vv) &= -P_\vu(\vv) + \vc \\
    &= -P_\vu(\vv) + (\vv - P_\vu(\vv)) \\
    &= \vv - 2P_\vu(\vv)
\end{align*}
Since $P_\vu$ is a linear map, so $H$ is a linear map.

## Reflection Matrix

$$ H(\vv) = \vv - 2P\vv $$
$$ Q = (I - 2P) = \begin{pmatrix}
uâ‚‚Â² - uâ‚Â² & -2uâ‚uâ‚‚ \\
-2uâ‚uâ‚‚ & uâ‚Â² - uâ‚‚Â²
\end{pmatrix} $$
using the identity $uâ‚Â² + uâ‚‚Â² = 1$

We can see this has the form $Q = \begin{pmatrix} a & b \\ b & -a \end{pmatrix}$ where
$aÂ² + bÂ² = 1$ and is a symmetric orthogonal matrix.

$$ QÂ² = (I - 2P)Â² = I - 4P + 4PÂ² = I $$
since $PÂ² = P$. Since $Q$ is symmetric, $Q = Q^T$ is therefore orthogonal.

# $O(2, â„)$-dichotomy

$$ \det: O(2, â„) â†’ â„^Ã— $$
$$ Q^T Q = I â‡’ \det(Q) = Â±1 $$

$$ Q^T Q = \begin{pmatrix}
aÂ² + bÂ²  &  ac + bd \\
ac + bd  &  cÂ² + dÂ²
\end{pmatrix} = I $$

```python
sage: var("a b c d")
(a, b, c, d)
sage: Q = matrix([[a, b], [c, d]])
sage: (Q.transpose() * Q).determinant().expand()
b^2*c^2 - 2*a*b*c*d + a^2*d^2
sage: 
sage: c = -b*d/a
sage: d2 = a^2 + b^2 - c^2
sage: d2 = d2.expand()
sage: d2
a^2 + b^2 - b^2*d^2/a^2
sage: e = d^2 - d2 == 0
sage: e
-a^2 - b^2 + b^2*d^2/a^2 + d^2 == 0
sage: e -= b^2*d^2/a^2 + d^2
sage: e
-a^2 - b^2 == -b^2*d^2/a^2 - d^2
sage: e /= (-b^2/a^2 - 1)
sage: e.simplify_full()
a^2 == d^2
```
Either $a = d$ or $a = -d$.

## $\ker(\det) = \Rot(2)$

Let $a = d â‡’ b = -c$ and $Qâ‚ = \begin{pmatrix}
a & -b \\
b & a \end{pmatrix}$.

Then $\det(Qâ‚) = aÂ² + bÂ² = 1$.
But this is the form of the rotation matrices
$R_Î¸ = \begin{pmatrix}
\cos Î¸ & -\sin Î¸ \\
\sin Î¸ & \cos Î¸
\end{pmatrix}$.

## $O(2, â„) - \Rot(2)$ are Reflections

Let $a = -d â‡’ b = c$ and $Qâ‚‚ = \begin{pmatrix}
a & b \\
b & -a \end{pmatrix}$.
Rewriting it
$$ Qâ‚‚ = \begin{pmatrix}
1 & 0 \\
0 & -1 \end{pmatrix} \begin{pmatrix}
a & -b \\
b & a \end{pmatrix} = \begin{pmatrix}
a & c \\
c & -a \end{pmatrix} $$
So $Qâ‚‚$ is a reflection and a rotation (coset of $Qâ‚$).

$\vv = (-c \;\; a + 1)^T, \vw = (a + 1 \;\; c)$ are orthogonal,
and $Q\vv = -\vv$, $Q\vw = \vw$. Normalize them to get an orthonormal basis.

# Exercise 7.3.9: Projection on Subspace Without Orthonormal Basis

$W$ is a subspace with $\dim W = n$ of $â„áµ$ with a basis $A = (\vaâ‚ â‹¯ \va_n)$.

## Let $\vx âˆˆ â„â¿$, when is $A\vx$ the projection of $\vx$ on $W$?

$$ \vx = âŸ¨\vaâ‚, A\vxâŸ© + â‹¯ + âŸ¨\va_n, A\vxâŸ© â‡’ A^T A\vx = \vx $$

## Prove $A^T A$ is invertible

Because we're working in $â„$, all dot products are non-negative
$$ âŸ¨Ax, AxâŸ© â‰¥ 0 $$
with the only zero being when $A\vx = \vzero$. But $\dim \col(A) = n â‡’ \dim \ker(A) = 0 â‡’ \vx = \vzero$.

Thus $\ker(A^TA) = \{ \vzero \}$.

## Let $P = A(A^T A)â»Â¹A^T$. Prove $P\vx âˆˆ W$ if $\vx âˆˆ â„áµ$, and $P\vx = \vx$ if $\vx âˆˆ W$

$$ T_A : â„â¿ â†’ â„áµ $$
$$ T_{A^T} : â„áµ â†’ â„â¿ $$
$$ T_{(A^T A)} : â„â¿ â†’ â„â¿ $$
$$ T_{(A^T A)â»Â¹A^T} : â„áµ â†’ â„â¿ $$
Let $\vy = T_{(A^T A)â»Â¹A^T}(\vx) âˆˆ â„â¿$, then since $\col(A) = W$, and $âˆ€ \vv âˆˆ â„â¿, A\vv âˆˆ W$,
so $A\vy âˆˆ W$.

We showed in the previous part that $\ker(A^T A) = \{ \vzero \} â‡’ T_{(A^T A)}$ is an isomorphism
with inverse $T_{(A^T A)â»Â¹} : â„â¿ â†’ â„â¿$ so
$$ T_{(A^T A)â»Â¹} T_{(A^T A)} = T_{(A^T A)â»Â¹} T_{A^T} T_A = e $$
which means that
$$ (A^T A)â»Â¹ A^T A = I_n $$
but $A = (\vaâ‚ â‹¯ \va_n)$ so we can left multiply by $A$
$$ (A^T A)â»Â¹ A^T A = (\veâ‚ â‹¯ \ve_n) $$
$$ (A^T A)â»Â¹ A^T \va_i = \ve_i â‡’ A(A^T A)â»Â¹ A^T \va_i = \va_i $$
which when viewing $\vx = râ‚ \vaâ‚ + â‹¯ + r_n \va_n âˆˆ W$, we prove the result.

## $\vv - P\vv$ is orthogonal to $W, âˆ€\vv âˆˆ â„áµ$

We can extend the basis $A = (\vaâ‚ â‹¯ \va_n)$ of $W$ to a basis of $â„áµ$.

Let $\vv = P\vv + \vc$. We see $P\vv âˆˆ W â‡’ \vc âˆ‰ W$. Since $â„áµ = W âŠ• W^âŸ‚$,
so $\vc âˆˆ W^âŸ‚$ and $âŸ¨\va_i, \vcâŸ© = \vzero$ but $\vc = \vv - P\vv$ and
$A^T \vc = \vzero$ is equivalent to $\begin{pmatrix}
âŸ¨\vaâ‚, \vcâŸ© \\
\vdots \\
âŸ¨\va_n, \vcâŸ©
\end{pmatrix} = \vzero$.

# Exercise 7.3.10

$$ Q = \begin{pmatrix}
a & b \\
b & -a
\end{pmatrix} : aÂ² + bÂ² = 1
\sim
Q = \begin{pmatrix}
1 - 2uâ‚Â² & -2uâ‚uâ‚‚ \\
-2uâ‚uâ‚‚   & 1 - 2uâ‚‚Â²
\end{pmatrix} : uâ‚Â² + uâ‚‚Â² = 1 $$
$$ X = \bar{K}[uâ‚, uâ‚‚] / âŸ¨uâ‚Â² + uâ‚‚Â² - 1âŸ© $$
$$ Y = \bar{K}[a, b] / âŸ¨aÂ² + bÂ² - 1âŸ© $$
$$ ğœ™ : X â†’ Y $$
$$ ğœ™(uâ‚, uâ‚‚) = (-uâ‚Â² + uâ‚‚Â², -2uâ‚uâ‚‚) $$
We prove $ğœ™$ is an isomorphism.

Surjective: $b = 0 â‡’$ either $uâ‚$ or $uâ‚‚$ is $0$.
But also $a = 0 â‡’ uâ‚‚$ or $uâ‚$ is $0$.
Thus $\ker ğœ™ = \{ \vzero \}$.

Injective:
\begin{align*}
a &= -uâ‚Â² + uâ‚‚Â² \\
    &= (uâ‚Â² + uâ‚‚Â² - 1) + 1 - 2uâ‚Â² \\
    &= 1 - 2uâ‚Â² \\
â‡’ uâ‚Â² &= \frac{1 - a}{2} \\
uâ‚‚ &= -\frac{b}{2uâ‚}
\end{align*}

*Show $Q$ is a reflection through $\mathbb{R}\begin{pmatrix} -uâ‚‚ \\ uâ‚ \end{pmatrix}$.*
$$ \vv = \begin{pmatrix} -uâ‚‚ \\ uâ‚ \end{pmatrix} $$
We want $\vw$ such that $âŸ¨\vv, \vwâŸ© = 0 â‡’ \vw = \begin{pmatrix} uâ‚ \\ uâ‚‚ \end{pmatrix}$.

Calculating, we see $Q\vv = \vv$ and $Q\vw = -\vw$, so $Q$ is the reflection around $\vv$.

# $â„³ á´®_B(Tâˆ˜S) = â„³ á´®_B(T) â„³ á´®_B(S)$

$$ T(ğ¯â±¼) = âˆ‘áµ¢â‚Œâ‚â¿ cáµ¢â±¼ ğ¯áµ¢ \quad â„³ á´®_B(T) = (cáµ¢â±¼) $$
$$ S(ğ¯â±¼) = âˆ‘áµ¢â‚Œâ‚â¿ dáµ¢â±¼ ğ¯áµ¢ \quad â„³ á´®_B(T) = (dáµ¢â±¼) $$
\begin{align*}
T(S(ğ¯â±¼)) &= dâ‚â±¼ T(ğ¯â‚) + â‹¯ + dâ‚™â±¼ T(ğ¯â‚™) \\
    &= dâ‚â±¼ (câ‚â‚ğ¯â‚ + â‹¯ + câ‚™â‚ğ¯â‚™) + â‹¯  + dâ‚™â±¼ (câ‚â‚™ğ¯â‚ + â‹¯  + câ‚™â‚™ğ¯â‚™) \\
    &= (ğ¯â‚ â‹¯  ğ¯â‚™) \M{
            dâ‚â±¼câ‚â‚ + â‹¯  + dâ‚™â±¼câ‚â‚™  ;
                â‹®                 ;
            dâ‚â±¼câ‚™â‚ + â‹¯  + dâ‚™â±¼câ‚™â‚™
        }
\end{align*}
\begin{align*}
(â„³ á´®_B(Tâˆ˜S))áµ¢â±¼ &= dâ‚â±¼cáµ¢â‚ + â‹¯  + dâ‚™â±¼cáµ¢â‚™ \\
               &= cáµ¢â‚dâ‚â±¼ + â‹¯  + cáµ¢â‚™dâ‚™â±¼ \\
\end{align*}
$$ âŸ¹  â„³ á´®_B(Tâˆ˜S) = â„³ á´®_B(T) â„³ á´®_B(S) $$

# Exercise 7.4.4: $P_ğš$ is Semisimple

$$ r = âŸ¨ğš, ğšâŸ© \quad P_ğš : â„Â² â†’ â„Â² $$
$$ âŸ¨ğš, ğ±âŸ© = aâ‚xâ‚ + aâ‚‚xâ‚‚ $$
$$ P = \frac{1}{r} \M{
    aâ‚Â²,  aâ‚aâ‚‚ ;
    aâ‚aâ‚‚, aâ‚‚Â²
} $$
$$ T(ğ®â‚) = Î»â‚ğ®â‚ $$
$$ T(ğ®â‚‚) = Î»â‚‚ğ®â‚‚ $$
$$ uâ‚â‚ \M{aâ‚Â² ; aâ‚aâ‚‚} + uâ‚‚â‚ \M{aâ‚aâ‚‚ ; aâ‚‚Â²} = Î»â‚ \M{uâ‚â‚ ; uâ‚‚â‚} $$
Let $(uâ‚â‚, uâ‚‚â‚) = (aâ‚, aâ‚‚)$
\begin{align*}
aâ‚Â³ + aâ‚aâ‚‚Â² &= Î»â‚aâ‚ \\
aâ‚Â²aâ‚‚ + aâ‚‚Â³ &= Î»â‚aâ‚‚ \\
âŸ¹  Î»â‚ &= aâ‚Â² + aâ‚‚Â²
\end{align*}

Now $ğ®â‚‚ = (-aâ‚‚, aâ‚)$ and $âŸ¨ğ®â‚, ğ®â‚‚âŸ© = 0$. Let $(uâ‚â‚‚, uâ‚‚â‚‚) = (-aâ‚‚, aâ‚)$
$$ uâ‚â‚‚ \M{aâ‚Â² ; aâ‚aâ‚‚} + uâ‚‚â‚‚ \M{aâ‚aâ‚‚ ; aâ‚‚Â²} = Î»â‚‚ \M{uâ‚â‚‚ ; uâ‚‚â‚‚} $$
\begin{align*}
-aâ‚Â²aâ‚‚ + aâ‚Â²aâ‚‚ &= -Î»â‚‚aâ‚ = 0 \\
-aâ‚aâ‚‚Â² + aâ‚aâ‚‚Â² &= -Î»â‚aâ‚‚ = 0 \\
âŸ¹  Î»â‚‚ &= 0
\end{align*}
so $P$ is diagonal with basis $\{ ğ®â‚, ğ®â‚‚ \}$
$$ â„³ á´®_B(P_ğš) = \M{
    aâ‚Â² + aâ‚‚Â²,  0   ;
    0,          0
} $$

# Dual Space

$$ V^* = L(V, ğ”½) $$
$$ ğ¯áµ¢^*(ğ¯â±¼) = Î´áµ¢â±¼ $$

## If $ğ¯â‚, â€¦, ğ¯â‚™$ is a basis of $V$, then $ğ¯â‚^*, â€¦, ğ¯â‚™^*$ is a basis of $V^*$

$$ f(ğ±) = aâ‚ğ¯â‚(ğ±) + â‹¯  + aâ‚™ğ¯â‚™(ğ±) = ğŸ(ğ±) $$
Evaluating this on the basis of $V$, we see
$$ f(ğ¯áµ¢) = aâ±¼ \textrm{ and } ğŸ(ğ¯áµ¢) = 0 âŸ¹  aâ±¼ = 0 $$

## $Î¦(ğ¯) = Ï†_ğ¯$ is an Isomorphism

Let $V$ be a finite-dimensional inner product space over $â„$.
$$ Ï†_ğ¯ : V â†’ â„ $$
$$ Ï†_ğ¯(ğ±) = âŸ¨ğ¯, ğ±âŸ© $$
$$ Î¦ : V â†’ V^* $$
$$ Î¦(ğ¯) = Ï†_ğ¯ $$
Since $\dim V = \dim V^*$, we only have to show $Î¦$ is injective.

$Î¦(ğ¯) = ğŸ âŸ¹  âŸ¨ğ¯, ğ±âŸ© = 0$ for all $ğ±$ including $ğ± = ğ¯$. So $\ker Î¦ = \{ ğŸ \}$.

# Characterization of the Determinant

## For any $Ïƒ âˆˆ S(n), ğ’Ÿ (C_{Ïƒ(1)}, â€¦, C_{Ïƒ(n)}) = \sgn(Ïƒ) ğ’Ÿ (Câ‚, â€¦, Câ‚™)$

Decompose $Ïƒ$ into a product of transpositions and apply $ğ’Ÿ (AE) = -ğ’Ÿ (A)$.

## Equivalence with $\det(C)$

Let $ğœâ±¼ = âˆ‘áµ¢ báµ¢â±¼ ğšáµ¢$ where $A = (ğšâ‚ â‹¯  ğšâ‚™), C = (ğšâ‚ â‹¯  ğšâ‚™)$, then applying multilinearity, we see
$$ ğ’Ÿ (C) = \left( âˆ‘_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) b_{Ïƒ(1),1} â‹¯  b_{Ïƒ(n),n} \right) ğ’Ÿ (A) $$
just by expanding out each column of $C$. Then set $A = I$ and get the desired result.

This also gives us the product rule for determinants since $C = AB$.

\def \Vss {V^{**}}
\def \Vs {V^*}

# Exercise 7.5.1: Isomorphism $Î” : V â†’ \Vss$

$$ ğ¯ âˆˆ V, \; f_ğ¯ = Î”(ğ¯) âˆˆ \Vss, \; f_ğ¯(Ï†) = Ï†(ğ¯) $$
Now let $ğ° âˆˆ \ker Î”$, then $g = Î”(ğ°)$ and $g(Ï†) = Ï†(ğ°) = 0$ for all $Ï† âˆˆ V^*$.

Since $ğ°$ can be written in terms of a basis, let $xÙâ‚, â€¦, xâ‚™ âˆˆ \Vs$ be the respective component functions so $xâ‚(ğ° ) = â‹¯  = xâ‚™(ğ° ) = 0$ and we see $ğ° = ğŸ$ so we prove injectivity.

$$ \dim L(V, ğ”½) = n = \dim V $$
so $\dim \Vss = \dim V$, and surjectivity follows from the above on $Î”$ being injective.

Lastly since $\Vs = L(V, ğ”½)$ consists of linear maps with additive property
\begin{align*}
Î”(ğ¯ + ğ° )(Ï†) &= Ï†(ğ¯ + ğ° ) \\
    &= Ï†(ğ¯) + ğœ™(ğ° ) \\
    &= Î”(ğ¯)(Ï†) + Î”(ğ°)(Ï†)
\end{align*}
proving $Î”$ is a homomorphism.

# Exercise 7.5.2: Adjoint Map $T^* : W^* â†’ V^*$

$$ T^*(Ï‰) = Ï‰âˆ˜T $$

## $T^*$ is Well-Defined and Linear

$W^* = L(W, ğ”½)$, and $T : V â†’ W$, so it follows their composition is well defined and linear.

## Matrix of $T^*$ wrt Dual Bases

$$ T(ğ¯â‚) = aâ‚â‚ğ°â‚ + â‹¯  + aâ‚˜â‚ğ°â‚˜ $$
$$ T^*(ğ°â‚^*)(ğ¯â‚) = ğ°â‚^*(T(ğ¯â‚)) = aâ‚â‚ $$
$$ â€¦ $$
$$ T^*(ğ°â‚˜^*)(ğ¯â‚) = aâ‚˜â‚ $$
$$ â€¦ $$
$$ T^*(ğ°â‚^*)(ğ¯â‚™) = aâ‚â‚™ $$
$$ â€¦ $$
$$ T^*(ğ°â‚˜^*)(ğ¯â‚™) = aâ‚˜â‚™ $$

$$ T^*(ğ°â‚^* â‹¯ ğ°â‚˜^*) = (ğ¯â‚^* â‹¯ ğ¯â‚™^*)B $$
$$ T^*(ğ°â‚^*)(ğ¯â‚) = aâ‚â‚ \; T^*(ğ°â‚^*)(ğ¯â‚™) = aâ‚â‚™ $$
$$ âŸ¹  T^*(ğ°â‚^*) = aâ‚â‚ğ¯â‚^* + â‹¯  + aâ‚â‚™ğ¯â‚™* $$
$$ B = \M{
    aâ‚â‚, â‹¯ , aâ‚â‚™  ;
       ,  â‹®,      ;
    aâ‚˜â‚, â‹¯ , aâ‚˜â‚™
} = Aáµ€ $$

## If $T$ is injective, then $T^*$ is surjective

$$ f âˆˆ V^* \; T^*(Ï‰) = Ï‰âˆ˜T $$
we prove there exists $Ï‰ âˆˆ W^*$ such that $f = Ï‰T âˆˆ V^*$.

Since $f$ is linear, it suffices to prove $ğ¯áµ¢^* âˆˆ \im T^*(W^*)$ for a single basis element of $V^*$.

Let $V^âŸ‚$ be the space orthogonal to $ğ¯áµ¢$. Then because $T$ is injective, $Tğ¯áµ¢ âˆ‰ V^âŸ‚$. Therefore we can define $Ï‰$ such that $Ï‰(Tğ¯áµ¢) = 1$ and $Ï‰(Tğ¯â±¼) = 0$ for all $j â‰  i$.

But then since $f(ğ¯áµ¢) = Ï‰âˆ˜T(ğ¯áµ¢) = 1$, and $f(ğ¯â±¼) = 0$ for $j â‰  i$, we have $f = ğ¯áµ¢^*$.

## If $T$ is surjective, then $T^*$ is injective

Let $T^*(zâ‚) = T^*(zâ‚‚)$, then $zâ‚T = zâ‚‚T âˆˆ V^*$.

Since $T$ is surjective, both $zâ‚, zâ‚‚ âˆˆ W^* = L(W, ğ”½)$ are defined over all of $W$, so $zâ‚(ğ°) = zâ‚‚(ğ°) \; âˆ€ ğ° âˆˆ W âŸ¹  zâ‚ = zâ‚‚$.

## $\dim \im(T) = \dim \im(T^*)$

$$ \dim(T) = \dim(A) = \dim(Aáµ€) = \dim(T^*) $$

## Natural Surjective Linear Map $S : W^* â†’ V^*$ for $V âŠ† W$

$$ S(ğ°^*)(ğ¯) = ğ°^*(ğ¯) $$

Write $ğ° = ğ¯ + ğœ$, then $S(ğ°^*) = S(ğ¯^*) + S(ğœ^*)$, which for all $ğ± âˆˆ V, S(ğ°*)(ğ±) = ğ¯*(ğ±)$

