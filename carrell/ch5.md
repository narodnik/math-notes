---
header-includes: |
    - \usepackage{mathabx}
    - \newcommand{\row}[1]{\textrm{row}({#1})}
    - \let\vec\mathbf
    - \DeclareMathOperator\sgn{sgn}
    - \DeclareMathOperator\rank{rank}
    - \DeclareMathOperator\adj{adj}
---

# Main Theorem

$$ \det : K^{nÃ—n} â†’ K $$

1. $\det(AB) = \det(A)\det(B)$
2. If $A = (a_{ij})$ is upper or lower triangular then $\det(A) = \prod_{i = 1}^n a_{ii}$.
3. If $E$ is a row swap matrix then $\det(E) = -1$.
4. $A$ is nonsingular iff $\det(A) â‰  0$.

Note that nonsingular means the rank of $A âˆˆ K^{nÃ—n}$ is $n$. For the matrix to be invertible $\rank(A) = n$ and $N(A) = \{ \vec{0} \}$.

## Simple Computation

Using row operations $Eâ‚, â€¦, E_k$, we can create an upper triangular matrix $U = Eâ‚ â‹¯ E_k A$ with $\det U = uâ‚â‚ â‹¯ u_{nn} â‡’ \det A = (uâ‚â‚ â‹¯ u_{nn}) / (\det(Eâ‚) â‹¯ \det(E_k))$.

# Signature of a Permutation

Define the signature $\sgn(Ïƒ)$ to be
$$ \sgn(Ïƒ) = \prod_{i < j} \frac{Ïƒ(i) - Ïƒ(j)}{i - j} $$

## $\sgn(Ïƒ) = Â±1 \quad âˆ€Ïƒ âˆˆ S(n)$

By swapping the arbitrary symbols $i, j$ we see
\begin{align*}
\prod_{i < j} \frac{Ïƒ(i) - Ïƒ(j)}{i - j} &= \prod_{j < i} \frac{Ïƒ(j) - Ïƒ(i)}{j - i} \\
    &= \prod_{j < i} \frac{Ïƒ(j) - Ïƒ(i)}{j - i} \\
    &= \prod_{j < i} \frac{Ïƒ(i) - Ïƒ(j)}{i - j} && \text{multiply prev line by (-1/-1)} \\
â‡’ (\sgn(Ïƒ))Â² &= \prod_{i < j} \frac{Ïƒ(i) - Ïƒ(j)}{i - j} \prod_{j < i} \frac{Ïƒ(i) - Ïƒ(j)}{i - j} \\
    &= \prod_{i â‰  j} \frac{Ïƒ(i) - Ïƒ(j)}{i - j}
\end{align*}
Expanding this out gives us all possible combos $i, j$, so $\sgn(Ïƒ)Â² = 1$.

## $\sgn(Ï„Ïƒ) = \sgn(Ï„)\sgn(Ïƒ)$

Let $N(Ïƒ) = \{(i, j) \; | \; i < j, Ïƒ(i) > Ïƒ(j)\}$, and $n(Ïƒ) = |N(Ïƒ)|$. Thus $n(Ïƒ)$ counts the
number of inversions in the set $D = \{(i, j) \; | \; i < j \}$. By the proposition above,
$$ \sgn(Ïƒ) = (-1)^{n(Ïƒ)} $$

Let $ÏƒD = \{(Ïƒ(i), Ïƒ(j)) \; | \; i < j\}$, then for all $k < l$, either $(k, l)$ or $(l, k) âˆˆ ÏƒD$.

Now apply $Ï„ÏƒD$ which contains either $(Ï„k, Ï„l)$ or $(Ï„l, Ï„k)$. Thus $Ï„$ inverts $n(Ï„)$ pairs,
and so $D â†’ ÏƒD â†’ Ï„ÏƒD$ has inverted $n(Ïƒ) + n(Ï„)$ pairs.

But $D â†’ (Ï„Ïƒ)D$ has inverted $n(Ï„Ïƒ)$ pairs.

We also see $(i, j) âˆˆ N(Ï„Ïƒ) â‡” (i, j) âˆˆ N(Ïƒ)$ or $(Ïƒ(i), Ïƒ(j)) âˆˆ N(Ï„)$.
And there is no pair $(i, j) âˆˆ N(Ï„Ïƒ) : (i, j) âˆˆ N(Ïƒ)$ and $(Ïƒ(i), Ïƒ(j)) âˆˆ N(Ï„)$ so it follows
$$ n(Ï„Ïƒ) = n(Ï„) + n(Ïƒ) $$

## $\sgn(Ïƒ) = \sgn(Ïƒâ»Â¹)$

Observe that $\sgn(Ïƒ) \sgn(Ïƒâ»Â¹) = \sgn(ÏƒÏƒâ»Â¹) = \sgn(e) = 1$. Then since $\sgn(Ïƒ), \sgn(Ïƒâ»Â¹) âˆˆ \{ -1, 1 \} â‡’ \sgn(Ïƒ) = \sgn(Ïƒâ»Â¹)$.

## $\sgn(Ïƒ)$ measures the number of transpositions

Note these facts:

* Every permutation is the product of distinct cycles.
* $\sgn(Ïƒ) = \sgn(Ïƒâ»Â¹)$ and $\sgn(ÏƒÏƒâ»Â¹) = 1$
* So the parity $\sgn(Ïƒ)$ is always consistent for all representations.

### Example: $Ïƒ = (37)$

Rewriting $Ïƒ$ as adjacent transpositions, we see
$$ Ïƒ = \begin{pmatrix}
â‹¯ & 3 & 4 & 5 & 6 & 7 & â‹¯ \\
â‹¯ & 4 & 3 & 5 & 6 & 7 & â‹¯ \\
â‹¯ & 4 & 5 & 3 & 6 & 7 & â‹¯ \\
â‹¯ & 4 & 5 & 6 & 3 & 7 & â‹¯ \\
â‹¯ & 4 & 5 & 6 & 7 & 3 & â‹¯ \\
â‹¯ & 4 & 5 & 7 & 6 & 3 & â‹¯ \\
â‹¯ & 4 & 7 & 5 & 6 & 3 & â‹¯ \\
â‹¯ & 7 & 4 & 5 & 6 & 3 & â‹¯ \\
\end{pmatrix} $$
where we first do $m - n = 7 - 3 = 4$ swaps corresponding to rows $2-5$.
Then we finally do $m - n - 1$ swaps corresponding to the remaining rows.

We can thus rewrite the cycle as
$$ Ïƒ = (47)(57)(67)(37)(36)(35)(34) $$
where we see
$$ Ïƒ = \begin{pmatrix}
â‹¯ & 3 & 4 & 5 & 6 & 7 & â‹¯ \\
â‹¯ & 7 & 4 & 5 & 6 & 3 & â‹¯ \\
\end{pmatrix} $$
as desired.

The total is $2(m - n) - 1$ adjacent transpositions.

### Correspondence between formulas

Let $Ïƒ = (mn)$, then
$$ \sgn(Ïƒ) = \prod_{i < j} \frac{Ïƒ(i) - Ïƒ(j)}{i - j} = (-1) $$
We can see by above that the parity of $Ïƒ$ can be evaluated by
counting the number of swaps for all $i < j$.
For a single transposition, this will be $\sgn(Ïƒ) = -1$.

Since $\sgn(Ïƒ)$ is multiplicative therefore $\sgn(Ï„) = \sgn(Ïƒâ‚)â‹¯\sgn(Ïƒ_k) = (-1)áµ$
where $Ï„ = Ïƒâ‚ â‹¯ Ïƒ_k$.

# Leibniz Formula

$$ \det(A) := \sum_{Ï€ âˆˆ S(n)} \sgn(Ï€) a_{Ï€(1)1} â‹¯ a_{Ï€(n)n} $$

When $U$ is upper (or lower) triangular then every permutation along rows or columns will end up including a 0. Hence $\det(U) = u_{11} â‹¯ u_{nn}$.

When $P = P_Î¼$ is a permutation matrix then $\det(P) = \sgn(Î¼)$. Recall $P_Î¼ = (\vec{e_{Î¼(1)}} â‹¯ \vec{e_{Î¼(n)}})$. Then $p_{Î¼(i)i} = 1$ for all $i$, but is $0$ otherwise. Therefore $\det(P) = \sgn(Î¼) p_{Î¼(1)1} â‹¯ p_{Î¼(n)n} = \sgn(Î¼)$.

## $\det(A^T) = \det(A)$

Observe that $Ïƒ(i) = j$ then $Ïƒâ»Â¹(j) = i$ which is bijective. So given the set of tuples $\{ (Ïƒ(1), 1), â€¦, (Ïƒ(n), n) \}$, then the set $\{ (1, Ïƒâ»Â¹(1)), â€¦, (n, Ïƒâ»Â¹(n)) \}$ is the same since $Ïƒ : \{ 1, â€¦, n \} â†’ \{ 1, â€¦, n \}$ is bijective.
$$ \sgn(Ïƒ) a_{Ïƒ(1)1} â‹¯ a_{Ïƒ(n)n} = \sgn(Ïƒ) a_{1Ïƒâ»Â¹(1)} â‹¯ a_{nÏƒâ»Â¹(n)} $$
Using the result that $\sgn(Ïƒ) = \sgn(Ïƒâ»Â¹)$, and relabelling $Ïƒâ»Â¹$ as $Ï„$, we get

\begin{align*}
\det(A) &= \sum_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) a_{Ïƒ(1)1} â‹¯ a_{Ïƒ(n)n} \\
        &= \sum_{Ï„ âˆˆ S(n)} \sgn(Ï„) a_{1Ï„(1)} â‹¯ a_{nÏ„(n)} \\
        &= \det(A^T)
\end{align*}

# Row Operations on the Determinant

## Multiply Row by $r â‡’ \det(EA) = r\det(A)$

Let $E$ by the matrix multiplying a single row by $r$, then $\det(E) = r$.

Likewise $\det(EA) = r\det(A)$ just by looking at the formula.

## Swap Rows $â‡’ \det(SA) = -\det(A)$

Let $B = SA$, and denote $S = P_Ï„$ where $Ï„$ is a transposition.

Since $S$ swaps rows, we can observe that $b_{ij} = a_{Ï„(i)j}$.
$$ \det(B) = \sum_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) b_{1Ïƒ(1)} â‹¯ b_{nÏƒ(n)} = \sum_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) a_{Ï„(1)Ïƒ(1)} â‹¯ a_{Ï„(n)Ïƒ(n)} $$
Now let $Î¼ = ÏƒÏ„$ and since $Ï„$ is a transposition $â‡’ Ïƒ = Î¼Ï„$
\begin{align*}
\det(B) &= \sum_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) a_{Ï„(1)Î¼Ï„(1)} â‹¯ a_{Ï„(n)Î¼Ï„(n)} \\
        &= \sum_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) a_{1Î¼(1)} â‹¯ a_{nÎ¼(n)} \\
        &= \sum_{Î¼ âˆˆ S(n)} \sgn(Î¼Ï„) a_{1Î¼(1)} â‹¯ a_{nÎ¼(n)} \\
        &= -\sum_{Î¼ âˆˆ S(n)} \sgn(Î¼) a_{1Î¼(1)} â‹¯ a_{nÎ¼(n)} \\
        &= -\det(A)
\end{align*}
We also see $\det(S) = -1$ since $\det(SI) = -\det(I) = -1$.

## Row Transvection $â‡’ \det(EA) = \det(A)$

The $i$th row of $EA$ is $\vec{a}_i + r\vec{a}_j$.
$$ (EA)_{ik} = a_{ik} + ra_{jk} $$
So we can see $\det(EA) = \det(A) + r\det(C)$, where $C$ has the property that rows $\vec{c}_i = \vec{c}_j$.

### $C$ has two rows the same $â‡’ \det(C) = 0$

Let $S$ be the row swap matrix for rows $i, j$. Then $\det(C) = \det(SC) = -\det(C) â‡’ 2\det(C) = 0 â‡’ \det(C) = 0$ (if the characteristic is not 2).

Using the transpose this also applies to columns.

### $\det(EA) = \det(A)$

\begin{align*}
\det(EA) &= \det(A) + r\det(C) \\
         &= \det(A)
\end{align*}

# $\det(A) â‰  0 â‡” A$ is Nonsingular

Write $A$ in reduced form using elementary matrices
$$ A_{\textrm{red}} = Eâ‚ â‹¯ E_k A $$
Then by the results above, we know the product formula is valid for elementary matrices
$$ \det(A_{\textrm{red}}) = \det(Eâ‚) â‹¯  \det(E_k) \det(A) $$
So $\det(A) â‰  0 â‡” \det(A_{\textrm{red}}) â‰  0$. But $A_{\textrm{red}}$ is upper triangular so $\det(A_{\textrm{red}}) â‰  0 â‡” A_{\textrm{red}} = I$.

## $\det(AB) = \det(A) \det(B)$ for Nonsingular $A, B$

Now we prove the product formula. First for nonsingular $A, B$, then $A_{\textrm{red}} = B_{\textrm{red}} = I$ and
\begin{align*}
AB &= Eâ‚ â‹¯ E_k A_{\textrm{red}} Fâ‚ â‹¯ F_j B_{\textrm{red}} \\
   &= Eâ‚ â‹¯ E_k Fâ‚ â‹¯ F_j \\
\end{align*}
$$ \det(AB) = \det(A) \det(B) $$
for nonsingular $A, B$.

## $\det(AB) = \det(A) \det(B)$ for Singular $A, B$

Finally to prove $\det(AB) = \det(A)\det(B)$ if $A$ (or $B$) is singular, we prove that $AB$ is singular.

Assume $AB$ is nonsingular. Then $(AB)â»Â¹ = Bâ»Â¹Aâ»Â¹$ exists and is nonsingular. Then $B(AB)â»Â¹$ (or $(AB)â»Â¹A$) also exists and is a nonsingular inverse of $A$ (or $B$).

So $AB$ is also singular, and hence $\det(A)\det(B) = 0 â‡’ \det(AB) = 0$.

# If $A = LPDU$ is Nonsingular, then $\det(A) = Â±\det(D)$

Use product formula

# Laplace Expansion

$$ \det(A) = \sum_{i = 1}^n (-1)â±âºÊ² a_{ij} \det(A_{ij}) $$
This is the laplace expansion along the $j$th column. Because $\det(A) = \det(A^T)$, we can also do the same expansion along the $i$th row instead.

Assume $j = 1$ then
\begin{align*}
\det(A) &= \sum_{Ïƒ âˆˆ S(n)} \sgn(Ïƒ) a_{Ïƒ(1)1} a_{Ïƒ(2)2} â‹¯  a_{Ïƒ(n)n} \\
        &= aâ‚â‚ \sum_{Ïƒ(1) = 1} \sgn(Ïƒ) a_{Ïƒ(2)2} â‹¯ a_{Ïƒ(n)n} + â‹¯ 
            + a_{n1} \sum_{Ïƒ(1) = n} \sgn(Ïƒ) a_{Ïƒ(2)2} â‹¯ a_{Ïƒ(n)n}
\end{align*}

Now we have $Ïƒ âˆˆ S(n)$ where $Ïƒ(1) = r$. Take $P_Ïƒ âˆˆ ğ”½^{nÃ—n}$ and delete column 1 and row $r$. Note that since every row and column contains a single 1, the new $P_Ïƒ' âˆˆ ğ”½^{(n-1)Ã—(n-1)}$ is also a valid permutation. So $P_Ïƒ' = P_{Ïƒ'}$ for some $Ïƒ âˆˆ S(n - 1)$.

Let $P_{Ïƒ'}$ take $t$ row swaps to become the identity $I_{n-1}$. Then $\sgn(Ïƒ') = \det(P_{Ïƒ'}) = (-1)áµ—$.

Adding back row $r$, and noting $Ïƒ(r) = 1$, we see that we require $r - 1$ row swaps to bring it to the first row. That means we need $t + r - 1$ row swaps to bring $P_Ïƒ$ to the identity $I_n$. So $\sgn(Ïƒ) = (-1)Ê³â»Â¹\sgn(Ïƒ')$

\begin{align*}
\sum_{Ïƒ(1) = r} \sgn(Ïƒ) a_{Ïƒ(2)2} â‹¯ a_{Ïƒ(n)n}
    &= \sum_{Ïƒ' âˆˆ S(n-1)} (-1)Ê³â»Â¹ \sgn(Ïƒ') a_{Ïƒ(2)2} â‹¯ a_{Ïƒ(n)n} \\
    &= (-1)Ê³â»Â¹ \det(A_{r1}) \\
    &= (-1)Ê³âºÂ¹ \det(A_{r1})
\end{align*}
where the last line we note $(-1)â»Ê² = (-1)âºÊ²$.

# Cramer's Rule (3x3 Case)

Let $M(A) âˆˆ ğ”½^{nÃ—n}$ be the matrix whose $ij$-entry is $(-1)â±âºÊ² \det(A_{ij})$.
The **adjoint** matrix of $A$ is $\adj(A) = M(A)^T$.

$$ \adj(A) = \begin{pmatrix}
\det(A_{11}) & -\det(A_{21}) & \det(A_{31}) \\
-\det(A_{12}) & \det(A_{22}) & -\det(A_{32}) \\
\det(A_{13}) & -\det(A_{23}) & \det(A_{33}) \\
\end{pmatrix} $$

Since we want to prove $Aâ»Â¹ = \frac{1}{\det(A)} \adj(A)$, we can also show $I = Aâ»Â¹A = \frac{1}{\det(A)} \adj(A) A$ or rather
$$ \det(A)I = \adj(A)A $$

$$ \adj(A)A = \begin{pmatrix}
\det(Aâ‚â‚) & -\det(Aâ‚‚â‚) & \det(Aâ‚ƒâ‚) \\
-\det(Aâ‚â‚‚) & \det(Aâ‚‚â‚‚) & -\det(Aâ‚ƒâ‚‚) \\
\det(Aâ‚â‚ƒ) & -\det(Aâ‚‚â‚ƒ) & \det(Aâ‚ƒâ‚ƒ) \\
\end{pmatrix}
\begin{pmatrix}
aâ‚â‚ & aâ‚â‚‚ & aâ‚â‚ƒ \\
aâ‚‚â‚ & aâ‚‚â‚‚ & aâ‚‚â‚ƒ \\
aâ‚ƒâ‚ & aâ‚ƒâ‚‚ & aâ‚ƒâ‚ƒ \\
\end{pmatrix} $$

Expanding the diagonal entries, we see
\begin{alignat*}{2}
(\adj(A)A)â‚â‚ &= aâ‚â‚\det(Aâ‚â‚) - aâ‚‚â‚\det(Aâ‚‚â‚) + aâ‚ƒâ‚\det(Aâ‚ƒâ‚) &&= \det(A) \\
(\adj(A)A)â‚‚â‚‚ &= aâ‚â‚‚\det(Aâ‚â‚‚) - aâ‚‚â‚‚\det(Aâ‚‚â‚‚) + aâ‚ƒâ‚‚\det(Aâ‚ƒâ‚‚) &&= \det(A) \\
(\adj(A)A)â‚ƒâ‚ƒ &= aâ‚â‚ƒ\det(Aâ‚â‚ƒ) - aâ‚‚â‚ƒ\det(Aâ‚‚â‚ƒ) + aâ‚ƒâ‚ƒ\det(Aâ‚ƒâ‚ƒ) &&= \det(A) \\
\end{alignat*}
The remaining non-diagonal entries $(\adj(A)A)_{ij}$ are of the form
\begin{align*}
(\adj(A)A)_{ij} &= \sum_{k=1}^n (\adj(A))_{ik} a_{kj} \\
    &= \sum_{k=1}^n (-1)^{k + i} a_{kj} \det(A_{ki}) \\
\end{align*}
Let $B = (A \leftsquigarrow^i \vec{a}_j)$ be the matrix, where we replace column $i$ in $A$ with column $j$. We can then see that $(\adj(A)A)_{ij} = \det(B)$ for $i â‰  j$. But $B$ has 2 columns that are the same so $(\adj(A)A)_{ij} = \det(B) = 0$.

So finally we have proved the relation and hence the inverse of $A$ by
$$ \det(A)I = \adj(A)A $$

# Exercise 5.1.5

Put $B, C$ in $LPDU$ form, then observe that the determinant is $\det(B)\det(C)$ by looking at the composition of diagonals.
