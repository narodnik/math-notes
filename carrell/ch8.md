---
header-includes: |
    - \let\vec\mathbf
    - \let\M\parenMatrixstack
    - \let\;\quad
    - \def\nullsp{\mathcal{N}}
    - \def\qed{\hfill\blacksquare}
    - \DeclareMathOperator\sgn{sgn}
    - \DeclareMathOperator\dim{dim}
    - \DeclareMathOperator\row{row}
    - \DeclareMathOperator\col{col}
    - \DeclareMathOperator\span{span}
    - \DeclareMathOperator\ker{ker}
    - \DeclareMathOperator\im{im}
    - \DeclareMathOperator\rank{rank}
    - \DeclareMathOperator\det{det}
    - \DeclareMathOperator\Rot{Rot}
    - \DeclareMathOperator\charpoly{charpoly}
    - \DeclareMathOperator\Tr{Tr}
    - \DeclareMathOperator\Re{Re}
    - \DeclareMathOperator\diagonal{diagonal}
    - \DeclareMathOperator\diag{diag}
---

# $Ïƒáµ¢(A)$ Formula

First note that the determinant is multi-linear, so we can write
$$ A = \M{
    aâ‚â‚ - x, â‹¯  ;
    â‹¯, aâ‚™â‚™ - x
} = \M{
    aâ‚â‚, â‹¯  ;
    â‹¯, aâ‚™â‚™
} - \M{
    x, â‹¯    ;
    â‹¯, aâ‚™â‚™
} - \M{
    x, â‹¯    ;
    â‹¯, aâ‚™â‚™
} + \M{
    x, â‹¯    ;
    â‹¯, x
} $$

Assume we have a single $aáµ¢áµ¢ = x$. We are only interested in the coefficient for $x$.
Fixing $Ï€$ and using the Leibniz formula, we get
$$ k = -x âˆ‘_{Ï€ âˆˆ S(n) : Ï€(i)=i} \sgn(Ï€) a_{1Ï€(1)} â‹¯ a_{(i-1)Ï€(i-1)} a_{(i+1)Ï€(i+1)} â‹¯ a_{nÏ€(n)} $$
Using the same argument as for the Laplace expansion, when we have $P_Ï€$, and delete the $i$th column and row,
then $P_Ï€' = P_{Ï€'}$ corresponds to a valid $Ï€' âˆˆ S(n - 1)$. Then $\sgn(Ï€) = \sgn(Ï€')$, since the element
at $Ï€(i) = i$ is already in the identity position.
$$ k = -x \det(Aáµ¢áµ¢) $$
Using the multilinearity we see
$$ Ïƒâ‚(A) = -x âˆ‘áµ¢â‚Œâ‚â¿ \det(Aáµ¢áµ¢) $$

We can generalize the argument above, by fixing $Ï€(iâ‚) = iâ‚, â€¦, Ï€(iâ‚˜) = iâ‚˜$. Then we sum over the permutations
for the remaining minor matrices $Aâ‚â‚™â‚‹â‚˜â‚â‚â‚™â‚‹â‚˜â‚$. Note that in the multilinear expansion, the leading unit is $(-1)áµ$.

Thus we get the generalized formula for characteristic polynomial.

# Exercises 8.1

## Ex 8.1.10: Matrices with Same Charpoly are Similar

Let $B = \M{3, -2; 2, -1}$, then $\charpoly(B) = \charpoly(I)$ but they are not similar since $S = PIPâ»Â¹ = I$.

## Ex 8.1.12: $0$ is an Eigenvalue of $A âŸº A$ is Singular

This means the constant term of charpoly is $0$. Therefore there is a row $i$ with $(A - xI)áµ¢áµ¢ = -x$ and all other entries $0$.
Which means $A$ has a row of all zeroes, and hence is singular.

## Ex 8.1.15: $A$ and $-A$ are Similar

Prove $\det(A) = \Tr(A) = 0$. Note $A = -PAPâ»Â¹ âŸ¹  \det(A) = -\det(P)\det(A)\det(Pâ»Â¹) = -\det(A) âŸ¹  \det(A) = 0$. Then using $\Tr(AB) = \Tr(BA)$, we can do $\Tr(A) = \Tr(-PAPâ»Â¹) = -\Tr(-PPâ»Â¹A) = -\Tr(A) âŸ¹  \Tr(A) = 0$.

$\det(A) = Î»â‚Î»â‚‚Î»â‚ƒ = 0$ implies one of the $Î»áµ¢$ is $0$.
Assume WLOG that $Î»â‚ = 0$.

Since $\Tr(A) = Î»â‚ + Î»â‚‚ + Î»â‚ƒ = Î»â‚‚ + Î»â‚ƒ = 0 âŸ¹  Î»â‚‚ = -Î»â‚ƒ$. But since they determine the roots of charpoly with real coefficients $Î»â‚‚ = \bar{Î»â‚ƒ}$ also. This means $\Re(Î»â‚ƒ) = \Re(Î»â‚‚) = 0$. Thus the 2 remaining eigenvalues are complex with non-real part. This means both eigenvectors are also complex.

## Ex 8.1.19: Similar Matrices

```sage
sage: K = GF(2)
sage: A = matrix(K, [[1, 0, 1], [0, 1, 0], [1, 0, 1]])
sage: A.charpoly().factor()
(x + 1) * x^2
sage: A.determinant()
0
sage: A = matrix(K, [[1,0,0,0,1],[0,1,0,1,0],[0,0,1,0,0],[0,1,0,1,0],[1,0,0,0,1]])
sage: A.determinant()
0
sage: A.trace()
1
sage: A.charpoly().factor()
(x + 1) * x^4
```
Any diagonal matrix similar to $Xâ‚$ has the same charpoly.
Since it is diagonal, then $\diagonal(D - xI) = \{ x + 1, x, x \}$ (in any order)
$âŸ¹ \diagonal(D) = \{ 1, 0, 0 \}$. Since the trace and determinant are unchanged
for similar matrices, we see that $\det(D) â‰  \det(A)$ which is a contradiction.

## Ex 8.1.20: Complex Eigenvalues of Real Matrix are Conjugate Pairs

$$ p(x) âˆˆ â„[x] : p(z) = 0 = \repr{p(z)} = p(\repr{z}) \textrm{ since } p(x) = \repr{p(x)} $$

## Ex 8.1.22: Real Eigenvalues of Real Orthogonal Matrix are $Â±1$

Orthogonal matrix means $âŸ¨ğ®áµ¢, ğ®â±¼âŸ© = 0$ for all $i â‰  j$ or equivalently $Qáµ€Q = I$.
$$ Qğ¯ = Î»ğ¯ \; ğ¯áµ€Qáµ€Qğ¯ = Î»Â²âŸ¨ğ¯, ğ¯âŸ© = âŸ¨ğ¯, ğ¯âŸ© âŸ¹  Î» = Â±1 $$

# $A = â„³ ^â„¬_â„¬(T)$ has Eigenpair $(Î¼, ğ±) âŸ¹ T$ has Eigenpair $(Î¼, â„¬ ğ±)$

$$ ğ¯ = âˆ‘ xáµ¢ ğ¯áµ¢ = (ğ¯â‚ â‹¯ ğ¯â‚™) ğ± $$
$$ (T(ğ¯â‚) â‹¯ T(ğ¯â‚™)) = (ğ¯â‚ â‹¯ ğ¯â‚™)A $$
Since $Ağ± = Î¼ğ±$
\begin{align*}
T(ğ¯) &= (T(ğ¯â‚) â‹¯ T(ğ¯â‚™)) ğ± \\
    &= Î¼(ğ¯â‚ â‹¯ ğ¯â‚™)ğ± \\
    &= Î¼ğ¯
\end{align*}

# $A âˆˆ ğ”½â¿$ has an Eigenbasis $ğ°â‚, â€¦, ğ°â‚™ âŸº A$ is Diagonalizable

$$ AP = (Ağ°â‚ â‹¯ Ağ°â‚™) = (ğ°â‚ â‹¯ ğ°â‚™) \diag(Î»â‚, â€¦, Î»â‚™) $$

# $T$ is Semisimple with Eigenbasis $(ğ¯â‚ â‹¯ ğ¯â‚™)P âŸº A$ is Diagonalizable

Let $â„¬ = \{ ğ¯â‚, â€¦, ğ¯â‚™ \}$ be a basis and $A = â„³ ^â„¬_â„¬ (T) = PDPâ»Â¹$. Since $T(ğ¯â‚ â‹¯ ğ¯â‚™) = (ğ¯â‚ â‹¯ ğ¯â‚™)A$ then
$$ (T(ğ¯â‚) â‹¯ T(ğ¯â‚™))P = (ğ¯â‚ â‹¯ ğ¯â‚™)PD $$
By 7.4.4, we have $TW = TVe = (TV)P$
$$ (T(ğ°â‚) â‹¯ T(ğ°â‚™)) = (T(ğ¯â‚) â‹¯ T(ğ¯â‚™)) â„³ ^â„¬_{â„¬ '} = (T(ğ¯â‚) â‹¯ T(ğ¯â‚™))P $$
but $A$ is defined by $T(ğ¯â‚ â‹¯ ğ¯â‚™) = (ğ¯â‚ â‹¯ ğ¯â‚™)A$ so
$$ T(ğ°â‚ â‹¯ ğ°â‚™) = (ğ¯â‚ â‹¯ ğ¯â‚™)AP = (ğ°â‚ â‹¯ ğ°â‚™)D $$

# Diagonalization via Eigenspace Decomposition

## Distinct Eigenvalues Produce Linearly Independent Eigenvectors

Let $Î±â‚ğ¯â‚ + Î±â‚‚ğ¯â‚‚ = ğŸ$. We want to show $Î±â‚ = Î±â‚‚ = 0$. Since $Ağ¯â‚ = Î»â‚$ and $Ağ¯â‚‚ = Î»â‚‚$,
so $A(Î±â‚ğ¯â‚ + Î±â‚‚ğ¯â‚‚) = Î±â‚Î»â‚ğ¯â‚ + Î±â‚‚Î»â‚‚ğ¯â‚‚ = ğŸ$. Subtracting $Î»â‚$ times the original equation,
we get
$$ Î±â‚‚(Î»â‚‚ - Î»â‚)ğ¯â‚‚ = ğŸ âŸ¹ Î±â‚‚ = 0 $$
since $Î»â‚ â‰  Î»â‚‚$. Then we conclude $Î±â‚ = 0$.

## Union of Subset of Independent Eigenvectors is Independent

**Proposition 8.10:** *Suppose $Î»â‚, â€¦, Î»â‚˜ âˆˆ ğ”½$ are distinct eigenvalues of $A âˆˆ ğ”½^{nÃ—n}$, and choose
a set $Sáµ¢$ of linearly independent eigenvectors in the eigenspace
$$ E_{Î»áµ¢} = \{ ğ¯ | (A - Î»áµ¢I)ğ¯ = ğŸ \} = ğ’© (A - Î»áµ¢I) $$
for all $1 â‰¤ i â‰¤ m$. Then the union of these linearly independent sets $S = Sâ‚ âˆª â‹¯ âˆª Sâ‚˜$ is also
linearly independent.*

Suppose $Aâ‚ = \{ \M{1, 0}áµ€ \}, Aâ‚‚ = \{ \M{0, 1}áµ€ \}, Aâ‚ƒ = \{ \M{1, 1}áµ€ \}$. Then $Aáµ¢ âˆ© Aâ±¼ = \{ ğŸ \}$
for all $i â‰  j$, but $Aâ‚ âˆª Aâ‚‚ âˆª Aâ‚ƒ$ is linearly dependent. So the result is not generally automatic
and needs to be proven for this specific case.

Denote the elements of $S$ as
$$ S = \{ ğ®â‚â½Â¹â¾, â€¦, ğ®â½Â¹â¾_{tâ‚}, ğ®â‚â½Â²â¾, â€¦, ğ®â½Â²â¾_{tâ‚‚}, â€¦, ğ®â‚â½áµâ¾, â€¦, ğ®â½áµâ¾_{tâ‚˜} \} $$
We want to show this set is linearly independent, so assume
$$ aâ‚â½Â¹â¾ğ®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ğ®â½Â¹â¾_{tâ‚} + aâ‚â½Â²â¾ğ®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ğ®â½Â²â¾_{tâ‚‚} + â‹¯ + aâ‚â½áµâ¾ ğ®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ğ®â½áµâ¾_{tâ‚˜} = ğŸ $$

\begin{align*}
Mâ‚ &= \{ ğ®â‚â½Â¹â¾, â€¦, ğ®â½Â¹â¾_{tâ‚} \} \\
Mâ‚‚ &= \{ ğ®â‚â½Â²â¾, â€¦, ğ®â½Â²â¾_{tâ‚‚} \} \\
    & â€¦ \\
Mâ‚˜ &= \{ ğ®â‚â½áµâ¾, â€¦, ğ®â½áµâ¾_{tâ‚˜} \}
\end{align*}

\begin{align*}
ğ¯â‚ &= aâ‚â½Â¹â¾ ğ®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ğ®â½Â¹â¾_{tâ‚} âˆˆ E_{Î»â‚} \\
ğ¯â‚‚ &= aâ‚â½Â²â¾ ğ®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ğ®â½Â²â¾_{tâ‚‚} âˆˆ E_{Î»â‚‚} \\
    & â€¦ \\
ğ¯â‚˜ &= aâ‚â½áµâ¾ ğ®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ğ®â½áµâ¾_{tâ‚˜} âˆˆ E_{Î»â‚ƒ}
\end{align*}
The elements $\{ ğ®â‚â½â±â¾, â€¦, ğ®â½â±â¾_{táµ¢} \} âˆˆ E_{Î»áµ¢}$ were chosen to be linearly independent from the set $E_{Î»áµ¢}$ and we have
\begin{align*}
ğŸ &= aâ‚â½Â¹â¾ğ®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ğ®â½Â¹â¾_{tâ‚} + aâ‚â½Â²â¾ğ®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ğ®â½Â²â¾_{tâ‚‚} + â‹¯ + aâ‚â½áµâ¾ ğ®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ğ®â½áµâ¾_{tâ‚˜} \\
    &= (aâ‚â½Â¹â¾ğ®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ğ®â½Â¹â¾_{tâ‚}) + (aâ‚â½Â²â¾ğ®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ğ®â½Â²â¾_{tâ‚‚}) + â‹¯ + (aâ‚â½áµâ¾ ğ®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ğ®â½áµâ¾_{tâ‚˜}) \\
    &= ğ¯â‚ + ğ¯â‚‚ + â‹¯ + ğ¯â‚˜ \\
    &= âˆ‘áµ¢â‚Œâ‚áµ ğ¯áµ¢
\end{align*}
Now we show all $ğ¯áµ¢ = ğŸ$.

WLOG suppose $ğ¯â‚ â‰  0$, then $ğ¯â‚ = -(ğ¯â‚‚ + â‹¯ + ğ¯â‚˜) â‰  0$.

Now select from the set $\{ ğ¯â‚‚, â€¦, ğ¯â‚˜ \}$, a subset that form a basis of $W = \span\{ğ¯â‚‚, â€¦, ğ¯â‚˜\}$. Relabelling as necessary, denote these
by $\{ ğ¯â‚‚, â€¦, ğ¯â‚— \}$ where $l = \dim(W)$. Each of these $ğ¯áµ¢ âˆˆ E_{Î»áµ¢}$ is an eigenvector with eigenvalue $Î»áµ¢$.

Since the eigenvalues are distinct $Î»â‚ â‰  Î»áµ¢$ for all $2 â‰¤ i â‰¤ l$. By the previous result that "distinct eigenvalues produce linearly
independent eigenvectors", this means that $bâ‚‚ = â‹¯ = bâ‚— = 0 âŸ¹ ğ¯â‚ = ğŸ$.

Reapplying the same logic, we prove all $ğ¯áµ¢ = ğŸ$. This means that
$$ ğ¯áµ¢ = aâ‚â½â±â¾ ğ®â‚â½â±â¾ + â‹¯ + aâ½â±â¾_{táµ¢} ğ®â½â±â¾_{táµ¢} = ğŸ $$
Since each $ğ®â±¼â½â±â¾$ is linearly independent within $\{ ğ®â‚â½â±â¾, â€¦, ğ®â½â±â¾_{táµ¢} \} âˆˆ E_{Î»áµ¢}$, this means $aâ‚â½â±â¾ = â‹¯ = aâ½â±â¾_{táµ¢} = 0$ for all $i$.

Therefore $S$ is linearly independent.

## Diagonalizable Condition $V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A)$

Let $Î»â‚, â€¦, Î»â‚˜$ denote the eigenvalues of $A$. $A$ is diagonalizable iff
$$ âˆ‘áµ¢â‚Œâ‚áµ \dim E_{Î»áµ¢}(A) = n $$
If $â„¬ áµ¢$ is a basis of $E_{Î»áµ¢}(A)$ then $â„¬ = â‹ƒ â„¬ áµ¢$ and
$$ V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A) $$

To prove this, use the previous proposition and the dimension theorem.

## $T$ is Semisimple $âŸ¹ (T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$

Since $T$ is semisimple, we have $V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A)$. So we just have to show that
for $ğ¯áµ¢ âˆˆ E_{Î»áµ¢}(A)$ that
$$ (T - Î»â‚I)â‹¯(T - Î»â‚˜I)ğ¯áµ¢ = ğŸ $$ {#eq:tl0}

Note that $(A - xI)(A - yI) = A - (x + y)I + xyI = (A - yI)(A - xI)$.
Therefore @eq:tl0 can be factored in the form $C(T - Î»áµ¢I)$
$$ âŸ¹ C(T - Î»áµ¢I)ğ¯áµ¢ = ğŸ $$

## Alternative Proof of $T$ is Semisimple $âŸ¹ (T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$

\begin{align*}
f(A) &= aâ‚™Aâ¿ + â‹¯ + aâ‚A + aâ‚€I \\
    &= aâ‚™(PDPâ»Â¹)â¿ + â‹¯ + aâ‚(PDPâ»Â¹) + aâ‚€(PPâ»Â¹) \\
    &= aâ‚™PDâ¿Pâ»Â¹ + â‹¯ + aâ‚PDPâ»Â¹ + aâ‚€PPâ»Â¹ \\
    &= P(aâ‚™Dâ¿ + â‹¯ + aâ‚D + aâ‚€I)Pâ»Â¹ \\
    &= Pf(D)Pâ»Â¹
\end{align*}
Now let $g(X) = (X - Î»â‚I)â‹¯(X - Î»â‚˜I)$ then
$$ g(A) = Pg(D)Pâ»Â¹ $$
but $D = \diag(Î»â‚, â€¦, Î»â‚˜) âŸ¹ (D - Î»â‚)â‹¯(D - Î»â‚˜) = O$ so
$$ g(A) = O $$

## $T$ is Semisimple $âŸ¸ (T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$

### $Pâˆ˜Q = O$ and $\ker(P) âˆ© \ker(Q) = \{ ğŸ \} âŸ¹ V = \ker(P) âŠ• \ker(Q)$

2. $PQ = O$
3. $\ker(P) âˆ© \ker(Q) = \{ ğŸ \}$

$$ \dim(V) = \dim \ker(Q) + \dim \im(Q) $$
but $Pâˆ˜Q = O$ so $\im(Q) âŠ† \ker(P)$ and so
$$ \dim \im(Q) = \dim(V) - \dim \ker(Q) â‰¤ \dim \ker(P) $$
$$ âŸ¹ \dim(V) â‰¤ \dim \ker(Q) + \dim \ker(P) $$
Since $\ker(P) âˆ© \ker(Q) = \{ ğŸ \}$, we have $\dim(\ker(P) âˆ© \ker(Q)) = 0$. By the Grasmann intersection formula
\begin{align*}
\dim(\ker(P) + \ker(Q)) &= \dim \ker(P) + \dim \ker(Q) - \dim(\ker(P) âˆ© \ker(Q)) \\
    &= \dim \ker(P) + \dim \ker(Q)
\end{align*}
So we see that $\dim(V) â‰¤ \dim(\ker(P) + \ker(Q))$, but $\ker(P) + \ker(Q) âŠ† V$ so
$$ V = \ker(P) + \ker(Q) $$
Lastly since $\ker(P) âˆ© \ker(Q) = \{ ğŸ \}$, we have
$$ V = \ker(P) âŠ• \ker(Q) $$

### Generalizing Above to $V = \ker(Tâ‚) âŠ• â‹¯ âŠ• \ker(Tâ‚™)$

**Lemma.** *Suppose $Tâ‚, â€¦, Tâ‚™ : V â†’ V$ are linear mappings that satisfy the following properties:*

1. $Tâ‚âˆ˜â‹¯âˆ˜Tâ‚™ = O$
2. $Táµ¢âˆ˜Tâ±¼ = Tâ±¼âˆ˜Táµ¢$ for all $i, j$
3. $\ker(Táµ¢) âˆ© \ker(Táµ¢â‚Šâ‚âˆ˜â‹¯âˆ˜Tâ‚™) = \{ ğŸ \}$ for all $i$

*Then $V = \ker(Tâ‚) âŠ• â‹¯ âŠ• \ker(Tâ‚™)$.*

*Proof.* Let $P = Tâ‚$ and $Q = Tâ‚‚ âˆ˜ â‹¯ âˆ˜ Tâ‚™$, then
$$ V = \ker(Tâ‚) âŠ• \ker(Tâ‚‚ âˆ˜ â‹¯ âˆ˜ Tâ‚™) $$
Now set $V' = \ker(Tâ‚‚ âˆ˜ â‹¯ âˆ˜ Tâ‚™), P = Tâ‚‚, Q = Tâ‚ƒ âˆ˜ â‹¯ âˆ˜ Tâ‚™$ and we get
$$ V = \ker(Tâ‚) âŠ• \ker(Tâ‚‚) âŠ• \ker(Tâ‚ƒ âˆ˜ â‹¯ âˆ˜ Tâ‚™) $$
Iterating we arrive at the conclusion
$$ V = \ker(Tâ‚) âŠ• â‹¯ âŠ• \ker(Tâ‚™) $$

### $T$ is Semisimple

Condition (1) is our starting assumption $(T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$.

For (2), we can also see that $(T - Î»â‚I)(T - Î»â‚‚I) = (T - Î»â‚‚I)(T - Î»â‚I)$.

Finally to prove (3), let $ğ± âˆˆ \ker(T - Î»áµ¢I)$ with $ğ± â‰  0$, then $(T - Î»â±¼I)ğ¯ = (Î»áµ¢ - Î»â±¼)ğ¯$ and so
$$ (T - Î»áµ¢â‚Šâ‚I) â‹¯ (T - Î»â‚˜I)ğ± = (Î»áµ¢ - Î»áµ¢â‚Šâ‚) â‹¯ (Î»áµ¢ - Î»â‚˜)ğ± â‰  0 $$
since $Î»áµ¢$ are distinct from all $Î»â±¼$.

Then by the previous lemma
$$ V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A) $$

# Exercises 8.3

## Ex 8.3.6

Computing the eigenvalues gives 2 distinct real values.

## Ex 8.3.7

Let $Î»â‚, Î»â‚‚$ be the eigenvalues for $A$. Then
$$ \M{a, b; c, d}\M{xâ‚; yâ‚} = Î»â‚ \M{xâ‚; yâ‚} \; \M{a, b; c, d}\M{xâ‚‚; yâ‚‚} = Î»â‚‚ \M{xâ‚‚; yâ‚‚} $$
$$ axâ‚ + byâ‚ = Î»â‚xâ‚ $$
$$ axâ‚‚ + byâ‚‚ = Î»â‚‚xâ‚‚ $$
$(x, yÙ)$ is in the first quadrant means $\frac{x}{yÙ} > 0$. Likewise $(x, y)$ is in the second/fourth quadrant means $\frac{x}{y} < 0$.
Rewriting both equations, we get
$$ \frac{xâ‚}{yâ‚} = -\frac{b}{a - Î»â‚} $$
$$ \frac{xâ‚‚}{yâ‚‚} = -\frac{b}{a - Î»â‚‚} $$
Noting that $\Tr(A) = a + d = Î»â‚ + Î»â‚‚$ and $\det(A) = ad - bc = Î»â‚Î»â‚‚$ we see
$$ \frac{xâ‚xâ‚‚}{yâ‚yâ‚‚} = -\frac{b}{c} < 0 $$
So one fraction is in the first quadrant, and another in the second.

## Ex 8.3.10

$$ ğ”½^{nÃ—n} = ğ”½^{nÃ—n}â‚› âŠ• ğ”½^{nÃ—n}â‚›â‚› $$
$$ A = \frac{1}{2}(A + Aáµ€) + \frac{1}{2}(A - Aáµ€) $$
When $A âˆˆ ğ”½^{nÃ—n}â‚›$ then $ğ•‹(A) = A$ so $Î»â‚ = 1$.

When $A âˆˆ ğ”½^{nÃ—n}â‚›â‚›$ then $ğ•‹(A) = -A$ so $Î»â‚ = -1$.

Now the basis for $ğ”½^{nÃ—n}â‚›$ and $ğ”½^{nÃ—n}â‚›â‚›$ are basis for $E_{Î»â‚}(ğ•‹)$ and $E_{Î»â‚›}(ğ•‹)$,
so $ğ•‹$ is semisimple.

## Ex 8.3.15

$\charpoly(U) = (x - Î»â‚)â‹¯(x - Î»â‚™)$ where $Î»â‚, â€¦, Î»â‚™$ are the diagonals of $U$.
Each space $\dim E_{Î»áµ¢}(U) â‰¥ 1 âŸ¹ \dim E_{Î»â‚} = â‹¯ = \dim E_{Î»â‚™} = 1$ and so $âˆ‘ \dim E_{Î»áµ¢} = n âŸ¹ V = E_{Î»â‚} âŠ• â‹¯ âŠ• E_{Î»â‚™}$
and thus $U$ is diagonalizable.

## Ex 8.3.17/18

$A = PDâ‚Pâ»Â¹, B = PDâ‚‚Pâ»Â¹ âŸ¹ AB = PDâ‚Dâ‚‚Pâ»Â¹ = PDâ‚‚Dâ‚Pâ»Â¹ = BA$.

Let $ğ¯ âˆˆ E_{Î»}(A)$, then $BAğ¯ = Î»Bğ¯ = ABğ¯ âŸ¹ Bğ¯ âˆˆ E_{Î»}(A)$.

Our goal is to find a common eigenbasis between $A$ and $B$.
Since $A$ and $B$ are diagonalizable, they both admit eigenbasis such that
\begin{align*}
V   &= E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚—}(A) \\
    &= E_{Î¼â‚}(B) âŠ• â‹¯ âŠ• E_{Î¼â‚˜}(B)
\end{align*}
viewing $A, B: V â†’ V$ as linear maps, we can take the restriction of
$$ B|_{E_{Î»áµ¢}(A)} : E_{Î»áµ¢}(A) â†’ E_{Î»áµ¢}(A) $$
which is a valid restriction since $B E_{Î»áµ¢}(A) âŠ† E_{Î»áµ¢}(A)$. Now let
\begin{align*}
V(Î», Î¼) &= \{ ğ¯ âˆˆ E_{Î»áµ¢}(A) | (B - Î¼)ğ¯ = ğŸ \} \\
        &= \{ ğ¯ âˆˆ V | (A - Î»)ğ¯ = (B - Î¼)ğ¯ = ğŸ \}
\end{align*}
$$ âŸ¹ E_{Î»áµ¢}(A) = V(Î»áµ¢, Î¼â‚) âŠ• â‹¯ âŠ• V(Î»áµ¢, Î¼â‚˜) $$
$$ V_{Î», Î¼ âˆˆ ğ”½} = â¨ V(Î», Î¼) $$
and since $\dim V = n$, we can pick an independent eigenbasis for both $A, B$.

## Ex 8.3.19

$U$ is diagonalizable when the $âˆ‘ \dim E_{Î»áµ¢}(U) = n$. Assume WLOG that $Î»$ is repeated twice on the diagonal for rows $i, j$,
then we want that $\dim E_{Î»}(U) = 2$. This means the row reduced form of $(U - Î»I)$, must have zero rows for $i, j$ which is
the same as saying $\dim (U - Î»I) = \dim E_Î»(U) = 2$.

Based off how row reduction works, see see that all vertical entries above $Î»$ should be 0, otherwise $(U - Î»I)$ has another
independent row vector.

# Flag Variety: Triangulation of Operators and Matrices

* Not every matrix is diagonalizable
* What we prove now is that for any operator on the complex finite dimensional vector space, there exists a basis in which
  its matrix is upper triangular.

Let $V$ be a finite dimensional vector space.

## Flag

Let $V$ be a finite dimensional vector space. A flag in $V$ is a sequence of nested subspaces
$$ Vâ‚ âŠ‚ Vâ‚‚ âŠ‚ â‹¯ âŠ‚ Vâ‚™ = V $$
such that $\dim Vâ‚– = k$.

A basis $â„¬$ of $V, â„¬ = \{ ğ¯â‚, â€¦, ğ¯â‚™ \}$ is called compatible with a given flag if $Vâ‚ = \span\{ ğ¯â‚ \}, â€¦, Vâ‚™ = \{ ğ¯â‚, â€¦, ğ¯â‚™ \}$.

## $T$-invariant flag

Let $T : V â†’ V$ be a linear operator. We say that a flag is $T$-invariant if $âˆ€k = 1, â€¦, n$, the subspace $Vâ‚–$ is invariant under $T$
$$ T(Vâ‚–) âŠ‚ Vâ‚– $$

Let $â„¬ = \{ ğ¯â‚, â€¦, ğ¯â‚™ \}$ be a basis of $V$ compatible with a $T$-invariant flag. What can we say about $T_â„¬$?

$T(ğ¯â‚) âˆˆ Vâ‚ âŸ¹ T(ğ¯â‚) = aâ‚â‚ ğ¯â‚$ by the defn of T-invariance. The first basis vector is therefore an eigenvector for our linear operator $T$.

Continuing we get
$$ T(ğ¯â‚‚) âˆˆ Vâ‚‚ âŸ¹ T(ğ¯â‚‚) = aâ‚â‚‚ğ¯â‚ + aâ‚‚â‚‚ğ¯â‚‚ $$
$$ T(ğ¯â‚ƒ) âˆˆ Vâ‚ƒ âŸ¹ T(ğ¯â‚ƒ) = aâ‚â‚ƒğ¯â‚ + aâ‚‚â‚ƒğ¯â‚‚ + aâ‚ƒâ‚ƒğ¯â‚ƒ $$
so putting this in a matrix we get
$$ T_â„¬ = \M{
    aâ‚â‚, aâ‚â‚‚, aâ‚â‚ƒ, â‹¯ ;
    0,   aâ‚‚â‚‚, aâ‚‚â‚ƒ, â‹¯ ;
    0,   0,   aâ‚ƒâ‚ƒ, â‹¯ ;
    0,   0,   0,   â‹¯ ;
    â‹®,   â‹®,   â‹®,
} $$
Conclusion: $T_â„¬$ is upper triangular.

## Theorem: Every linear operator $T: V â†’ V$ on a complex finite-dimensional vector space $V$ has an invariant flag

Corollary: for a square complex matrix $A$ there exist an upper-triangular matrix $S$ and an invertible matrix $C$ such that
$$ A = CSCâ»Â¹ $$

Proof of corollary: Let $â„¬$ be a basis compatible with an $A$-invariant flag, then we get $A = C^{\mathcal{E}}_â„¬ T_â„¬ C^â„¬_{\mathcal{E}}$.

Proof by induction on $n = \dim V$: $n = 1$ then $Vâ‚ = V$, then we see $T(V)$ is automatically invariant.

Now assume theorem holds for operators on $(n - 1)$-dimensional vector spaces over $â„‚$.
We know that any operator on a complex vector space of $\dim n â‰¥ 1$ has an eigenvector: $T(ğ¯) = Î»ğ¯$.

Set $ğ¯â‚ = ğ¯$, take $Vâ‚ = \span\{ğ¯â‚\}$, then since it is an eigenspace, it is invariant and $T(ğ¯â‚) âŠ‚ Vâ‚$.

By the 3rd homomorphism theorem, there exists an operator $T'$
$$ T' : V/Vâ‚ â†’ V/Vâ‚ $$
such that the following diagram commutes
\begin{tikzcd}
    V \arrow[r, "T"] \arrow[d, "P"]
        & V \arrow[d, "P"] \\
    V/Vâ‚ \arrow[r, dotted, "T'"]
        & V/Vâ‚
\end{tikzcd}
Now let $W = V/Vâ‚$, then $\dim W = n - 1$. By induction then $T'$ has an invariant flag in $W$.
Let $\{ ğ°â‚, â€¦, ğ°â‚™â‚‹â‚ \}$ be a basis of $W$, compatible with $T'$-invariant flag.
Then we can write these in terms of equivalence classes $ğ°â‚ = [ğ¯â‚‚], â€¦, ğ°â‚™â‚‹â‚ = [ğ¯â‚™]$.
Now we have a basis in the 1-dim space $Vâ‚$, and a basis in the quotient space $W$, and we have the result
that describes the relation basis in $V$ with the basis of the subspace and the basis of the quotient space.

Then $\{ ğ¯â‚, â€¦, ğ¯â‚™ \}$ is a basis of $V$, because $ğ¯â‚$ is a basis in the subspace, and the equivalence classes
of $ğ¯â‚‚, â€¦, ğ¯â‚™$ form a basis of the quotient space $W$.

### Claim: $\{ ğ¯â‚, â€¦, ğ¯â‚™ \}$ is a basis compatible with a $T$-invariant flag

$$ Vâ‚ = \span\{ ğ¯â‚ \}, â€¦, Vâ‚™ = \span\{ ğ¯â‚, â€¦, ğ¯â‚™ \} $$
We need to prove $T(ğ¯â‚–)$ is a linear combination of $ğ¯â‚, â€¦, ğ¯â‚–$ for $k = 1, â€¦, n$.

We know this holds for $k = 1$ since $T(ğ¯â‚) = Î»ğ¯â‚$.

We know $T'([ğ¯â‚–])$ is a linear combination of $[ğ¯â‚‚], â€¦, [ğ¯â‚–]$ since $\{ ğ°â‚, â€¦, ğ°â‚™â‚‹â‚ \}$ form a basis of $W$.
$$ T'([ğ¯â‚–]) = câ‚‚[ğ¯â‚‚] + â‹¯ + câ‚–[ğ¯â‚–]$$
$$ T'([ğ¯â‚–]) = [T(ğ¯â‚–)] âŸ¹ [T(ğ¯â‚–)] - câ‚‚[ğ¯â‚‚] - â‹¯ - câ‚–[ğ¯â‚–] = [T(ğ¯â‚–) - câ‚‚ğ¯â‚‚ - â‹¯ - câ‚–ğ¯â‚–] = [0] $$
$$ T(ğ¯â‚–) - câ‚‚ğ¯â‚‚ - â‹¯ - câ‚–ğ¯â‚– âˆˆ Vâ‚ âŸ¹ âˆƒcâ‚ âˆˆ â„‚ : T(ğ¯â‚–) - câ‚‚ğ¯â‚‚ - â‹¯ - câ‚–ğ¯â‚– = câ‚ğ¯â‚ $$
so we see $T(ğ¯â‚–) = câ‚ğ¯â‚ + â‹¯ + câ‚–ğ¯â‚–$.

# Proof of Cayley-Hamilton

$n = 1$ is trivial, so assume result is true for $n - 1$ given $n > 1$.

$(Î»â‚, ğ¯â‚)$ is an eigenpair for $A$. Extend $ğ¯â‚$ to a basis $â„¬$ of $V$. Then $A$ is similar to $B = â„³ ^â„¬_â„¬ (T_A)$ and
$p_A(x) = p_B(x)$.
$$ B = \M{
    Î»â‚, *, â‹¯, *;
    0,   ,  ,  ;
    â‹®,   , Bâ‚, ;
    0,   ,   ,
} $$

Now we note the eigenvalues of $Bâ‚$ are in $ğ”½$ since
\begin{align*}
p_A(x) &= p_B(x) \\
    &= \det(B - xIâ‚™) \\
    &= (Î»â‚ - x)\det(Bâ‚ - xIâ‚™â‚‹â‚) \\
    &= (Î»â‚ - x) p_{Bâ‚}(x)
\end{align*}
and we see they are the eigenvalues $\{ Î»â‚‚, â€¦, Î»â‚™ \}$ of $B$.

We equivalently calculate whether $p_B(B) = O âŸ¹ p_A(A) = O$.
We first note a general fact about matrices
$$ \M{
    câ‚, *, â‹¯, *;
    0,   ,  ,  ;
    â‹®,   , Câ‚, ;
    0,   ,   ,
} \M{
    câ‚‚, *, â‹¯, *;
    0,   ,  ,  ;
    â‹®,   , Câ‚‚, ;
    0,   ,   ,
} = \M{
    câ‚câ‚‚, *, â‹¯, *;
    0,     ,  ,  ;
    â‹®,     , Câ‚Câ‚‚, ;
    0,     ,  ,
} $$

We now wish to calculate $p_B(B)$ by lifting $p_B(x)$ with the map
$$ ğœ™ : ğ”½[x] â†’ ğ”½[B] $$
\begin{align*}
ğœ™(p_B(x)) &= ğœ™((-1)â¿(x - Î»â‚)â‹¯(x - Î»â‚™)) \\
    &= (-1)â¿(B - Î»â‚Iâ‚™)p_{Bâ‚}(Bâ‚) \\
    &= (-1)â¿ \M{
    0, *, â‹¯, *;
    0,  ,  ,  ;
    â‹®,  , Bâ‚ - Î»â‚Iâ‚™â‚‹â‚, ;
    0,  ,  ,
} \M{
    *, *, â‹¯, *;
    0,  ,  ,  ;
    â‹®,  , p_{Bâ‚}(Bâ‚),  ;
    0,  ,  ,
} \\
    &= (-1)â¿ \M{
    0, *, â‹¯, *;
    0,  ,  ,  ;
    â‹®,  , Bâ‚ - Î»â‚Iâ‚™â‚‹â‚, ;
    0,  ,  ,
} \M{
    *, *, â‹¯, *;
    0,  ,  ,  ;
    â‹®,  , O,  ;
    0,  ,  ,
} \\
    &= O
\end{align*}
since $p_{Bâ‚}(Bâ‚) = O$ by the induction hypothesis.

# Hermitian Principal Axis Theorem (also called Spectral Theorem)

## $V = \ker(T) âŠ• \im(T)$

We prove $\im(T) âŠ‚ \ker(T)^âŸ‚$. Let $ğ° âˆˆ \ker(T)$ and $T(ğ±) âˆˆ \im(T)$ then
$$ âŸ¨ğ°, T(ğ±)âŸ© = âŸ¨T(ğ°), ğ±âŸ© = ğŸ $$
so $T(x) âˆˆ \ker(T)^âŸ‚$ which means $\im(T) âŠ‚ \ker(T)^âŸ‚$.

Lets note two theorems. The first concerns a subspace and its orthogonal complement,
$V = W âŠ• W^âŸ‚$.
Secondly the rank-nullity theorem
$\dim V = \dim \ker(T) + \dim \im(T)$.

Now apply both theorems to get
\begin{align*}
\dim V &= \dim \ker(T) + \dim \ker(T)^âŸ‚ \\
    &= \dim \ker(T) + \dim \im(T)
\end{align*}
$$ âŸ¹ \dim \ker(T)^âŸ‚ = \dim \im(T) $$
and since $\im(T) âŠ‚ \ker(T)^âŸ‚$ so
$$ \ker(T)^âŸ‚  = \im(T) $$

## Principal Axis Theorem: Orthonormal Basis of $V$ from Eigenvectors of $T$

Assume true for $n - 1$, let $\dim V = n$.
Let $Î» âˆˆ â„$ and $S = T - Î»I$.

If $V = \ker(S)$ then we are done, so assume $V â‰  \ker(S)$.

$S$ is self-adjoint since
\begin{align*}
âŸ¨S(ğ¯â‚), ğ¯â‚‚âŸ© &= âŸ¨Tğ¯â‚ - Î»ğ¯â‚, ğ¯â‚‚âŸ© \\
    &= âŸ¨Tğ¯â‚, ğ¯â‚‚âŸ© - Î»âŸ¨ğ¯â‚, ğ¯â‚‚âŸ© \\
    &= âŸ¨ğ¯â‚, Tğ¯â‚‚âŸ© - âŸ¨ğ¯â‚, Î»ğ¯â‚‚âŸ© \\
    &= âŸ¨ğ¯â‚, S(ğ¯â‚‚)âŸ©
\end{align*}
so from the previous proposition we see
$$ V = \ker(S) âŠ• \im(S) $$

Observe that $T(\ker(S)) âŠ‚ \ker(S)$ and $T(\im(S)) âŠ‚ \im(S)$ so we can construct restrictions
of $T$ to both of these subspaces.

Since $0 < \dim \ker(S) â‰¤ n - 1$ and $0 < \dim \im(S) â‰¤ n - 1$ we can apply the induction
to both of these subspaces.

# Formula for $P_Î»$ when $A$ is Symmetric (Alternative)

This is similar to the lagrange interpolation trick.
$$ P_Î» = âˆ_{Î¼ â‰  Î»} \frac{A - Î¼I}{Î» - Î¼} $$

Let the eigenpairs be $(Î», ğ¯_Î»)$ and $(Î¼, ğ¯_Î¼)$. Applying $P_Î»$ we see
\begin{align*}
P_Î»(ğ¯_Î») &= âˆ_{Î¼ â‰  Î»} \frac{Ağ¯_Î» - Î¼ğ¯_Î»}{Î» - Î¼} \\
    &= âˆ_{Î¼ â‰  Î»} \frac{(Î» - Î¼)ğ¯_Î»}{Î» - Î¼} \\
    &= ğ¯_Î»
\end{align*}
\begin{align*}
P_Î»(ğ¯_Î¼) &= âˆ_{Î¼ â‰  Î»} \frac{Ağ¯_Î¼ - Î¼ğ¯_Î¼}{Î» - Î¼} \\
    &= âˆ_{Î¼ â‰  Î»} \frac{(Î¼ - Î¼)ğ¯_Î»}{Î» - Î¼} \\
    &= ğŸ
\end{align*}
Therefore we get that $P_Î»Â² = P_Î»$ by the above identities.

Then also for any two distinct eigenvalues we have $P_Î» P_Î¼ = P_Î¼ P_Î» = O$.

Lastly since $A = Aáµ€$ is symmetric, it follows from our definition that $P_Î» = P_Î»áµ€$.

The formula given in the book when we have the orthonomal eigenvector $ğ®_Î»$ is simply
$P_Î» = ğ®_Î» ğ®_Î»áµ€$ which is obtained by examining the formula
$P_Î»(ğ¯) = âŸ¨ğ®_Î», ğ¯âŸ© ğ®_Î» = ğ®_Î» (ğ®_Î»áµ€ ğ¯) = (ğ®_Î» ğ®_Î»áµ€) ğ¯$.

# Exercises 8.4

## Ex 8.4.3

*False proof of Cayley-Hamilton: $p_A(A) = \det(A - A) = 0$*

## Ex 8.4.4

$$ A = \M{a, b; c, d} $$
$A$ is nilpotent means $AÂ² = O$, so $AÂ² - \Tr(A)A + \det(A)I = O âŸ¹ \det(A)I = \Tr(A)A$.

```python
sage: var("a b c d")
(a, b, c, d)
sage: A = matrix([[a, b], [c, d]])
sage: C = (-A.trace()*A + A.det()*matrix.identity(2))
sage: e1 = C[0][0].expand() == 0
sage: e2 = C[0][1].expand() == 0
sage: e3 = C[1][0].expand() == 0
sage: e4 = C[1][1].expand() == 0
sage: e1
-a^2 - b*c == 0
sage: e2
-a*b - b*d == 0
sage: e3
-a*c - c*d == 0
sage: e4
-b*c - d^2 == 0
sage: e1 - e4
-a^2 + d^2 == 0
sage: # so a = +/-d
sage: e2 + b*d
-a*b == b*d
sage: # => a = -d
sage: e1.subs({-a^2: -a*(-d)})
-b*c + a*d == 0
sage: # so det(A) == 0
sage: # and since a = -d => a + d = Tr(A) == 0
```

## Ex 8.4.5

$Aáµ = O$ and let $(Î», ğ¯)$ be an eigenpair for $A$, then $Aáµğ¯ = Î»áµğ¯ = 0 âŸ¹ Î» = 0$.

And if all eigenvalues are 0, then $p(x) = xâ¿ âŸ¹ p(A) = Aâ¿ = O$.

## Ex 8.4.6

Assume $A = PDPâ»Â¹$ is diagonalizable, then $Aáµ = PDáµPâ»Â¹ = O âŸ¹ D = O$ and so $A = O$.

## Ex 8.4.7

$Aáµ = O âŸº Aâ¿ğ”½â¿ = \{ ğŸ \}$ so $\dim ğ’© (A) > 0$.
Let $ğ®â‚, â€¦, ğ®â‚›$ be a basis of $ğ’© (A)$ and extend it to a basis of $\col(A)$, then we see
$$ ğ”½â¿ = ğ®â‚ âŠ• â‹¯ âŠ• ğ®â‚› âŠ• ğ°â‚ âŠ• â‹¯ âŠ• ğ°â‚œ $$
now let $k$ be minimal such that $Aáµ = O$ and $k > n$, then we have a chain
$$ Aáµğ”½â¿ = \{ 0 \} âŠ‚ â‹¯ âŠ‚ Ağ”½â¿ âŠ‚ ğ”½â¿ $$
with
$$ \dim Aáµğ”½â¿ = 0 < â‹¯ < \dim Ağ”½â¿ \dim ğ”½â¿ = n $$
viewing $T_A(ğ¯) = Ağ¯$, we see $\dim Ağ”½â¿ = \dim T_A = \rank(A)$.

But there are $n$ maximum steps, so $k < n$.

## Ex 8.4.12

$A$ forms a 1d vector space over $ğ”½^{nÃ—n}$, so if $AÂ² âˆ‰ \span\{A\}$ then $\{A, AÂ²\}$ are independent and so on.
So $Tâ°, â€¦, Táµâ»Â¹$ are independent. Also we can just view powers of $T$ as a basis for a subspace in $â„’(V, V)$.

Note that $â„’(V, W) = (\dim V)(\dim W)$ although we only use the weaker property that $â„’(V, V)$ is finite dimension
given that $V$ is finite dimension.

## Ex 8.5.2

*Proposition 8.22. A linear mapping $T : V â†’ V$ is self-adjoint iff its matrix with respect to an arbitrary
orthonormal basis of $V$ is symmetric.*

$$ âŸ¨ğ±, Ağ²âŸ© = âŸ¨Ağ±, ğ²âŸ© = âŸ¨ğ±, Aáµ€ğ²âŸ© $$
Then using the orthonormal basis $ğ®â‚, â€¦, ğ®â‚™$ we see
$$ Ağ®â±¼ = aâ‚â±¼ğ®â‚ + â‹¯ + aâ‚™â±¼ğ®â‚™ $$
$$ Aáµ€ğ®â±¼ = bâ‚â±¼ğ®â‚ + â‹¯ + bâ‚™â±¼ğ®â‚™ $$
And then evaluating $âŸ¨ğ®áµ¢, Ağ®â±¼âŸ© = âŸ¨ğ®â±¼, Aáµ€ğ®â±¼âŸ©$, we see $A = Aáµ€$.

## Ex 8.5.6

$$ W = \span\{ ğ®â‚, ğ®â‚‚ \} \; ğ®â‚ƒ âˆˆ W^âŸ‚ \; Páµ¢ = ğ®áµ¢ğ®áµ¢áµ€ $$
$$ B = Î»â‚Pâ‚ + Î»â‚‚Pâ‚‚ + Î»â‚ƒPâ‚ƒ \textrm{ where } Î»â‚ƒ = \det(B) / (Î»â‚Î»â‚‚) $$

## Ex 8.5.7

Product of symmetric matrices is only symmetric when $AB = BA$ since $AB = Aáµ€Báµ€ = (BA)áµ€$.

Rearrange $A$ and $B$ to get $D = Qáµ€AQ = Ráµ€BR$ so $A = QRáµ€BRQáµ€ = (QRáµ€)B(QRáµ€)áµ€$ so is true.

## Ex 8.5.8

Let $(Î»â‚, ğ¯â‚)$ and $(Î»â‚‚, ğ¯â‚‚)$ be eigenpairs of $A$. Since $\det(A) = 0$, one of the eigenvalues
of $A$ is 0. Because $ğ¯â‚, ğ¯â‚‚$ lie in the image of $T_A$, so $Î»â‚, Î»â‚‚ â‰  0$ and $Î»â‚ƒ = 0$. So we see
$$ A = Î»â‚Pâ‚ + Î»â‚‚Pâ‚‚ $$
Making $ğ¯â‚, ğ¯â‚‚$ into orthonormal vectors and constructing $Pâ‚, Pâ‚‚$, we see that
$$ \Tr(A) = Î»â‚ + Î»â‚‚ = 2Î»â‚ + 3Î»â‚‚ = 4 $$
then we get $Î»â‚ = Î»â‚‚$
$$ A = \M{ 1, 0, 1; 0, 2, 0; 1, 0, 1 } \; Î»â‚ = Î»â‚‚ = 2 \; Î»â‚ƒ = 0 \; ğ¯â‚ƒ = \M{1, 0, -1}áµ€ $$

## Ex 8.5.11

$$ W = \span\{ğ°â‚, â€¦, ğ°â‚›\} \; W^âŸ‚ = \span\{ ğ¯â‚, â€¦, ğ¯â‚œ \} $$
with $s + t = n$ and $P_W = ğ°â‚ğ°â‚áµ€ + â‹¯ + ğ°â‚›ğ°â‚›áµ€$ where all $ğ°áµ¢, ğ¯áµ¢$ are normal.
Then $Q = \M{ğ°â‚, â‹¯, ğ°â‚›, ğ¯â‚, â‹¯, ğ¯â‚œ}, Dâ‚ = \diag(\underbrace{1, â€¦, 1}_s, \underbrace{0, â€¦, 0}_t)$
and $Dâ‚‚ = \diag(\underbrace{0, â€¦, 0}_s, \underbrace{1, â€¦, 1}_t)$.
Then $P_W = QDâ‚Qáµ€, P_{W^âŸ‚} = QDâ‚‚Qáµ€$.

## Ex 8.5.12

$\det(A) = Î»â‚Î»â‚‚Î»â‚ƒ, \Tr(A) = Î»â‚ + Î»â‚‚ + Î»â‚ƒ = Î»â‚ƒ âŸ¹ Î»â‚ = -Î»â‚‚$ so $Î»â‚Î»â‚‚ < 0$.
Since $Î»â‚ƒ âˆˆ â„$ so $Î»â‚ƒÂ² > 0$ or $\det(A)\Tr(A) = Î»â‚Î»â‚‚Î»â‚ƒÂ² < 0$.

## Ex 8.5.13

For (i), factor the determinant and notice that $Tr(A)$ is a term, so $\det(A) = 0$.

The eigenvalues for $A$ are $0, \Tr(A)$. The eigenvector for $Î»â‚ = \Tr(A)$ is $\M{1,1,1}áµ€$
and the rest is easy.

## Ex 8.5.23

$$ \det(R - xI) = (x - \cos(Î¸))Â² + \sinÂ²(Î¸) âŸ¹ Î» = \cos(Î¸) Â± i\sin(Î¸) $$

## Ex 8.5.26 (ii)

$$ âŸ¨Uğ¯, Uğ¯âŸ© = âŸ¨Î»ğ¯, Î»ğ¯âŸ© = âŸ¨ğ¯, Uá´´Uğ¯âŸ© = âŸ¨ğ¯, ğ¯âŸ© âŸ¹ |Î»| = 1 $$
so since $\det(U) = 1$, either $Î»â‚ = Î»â‚‚ = 1$ or $Î»â‚ = Î»â‚‚ = -1$.

## Ex 8.5.32

$$ Î¦(aâ‚ğ®â‚ + â‹¯ + aâ‚™ğ®â‚™) = \M{aâ‚, â‹¯, aâ‚™}áµ€ $$
where $\{ ğ®â‚, â€¦, ğ®â‚™ \}$ is the basis for $V$.

Eigenvectors within the same eigenspace are easily orthogonalized using the Gram-Schmidt method.

Now let $ğ¯â‚ âˆˆ E_{Î»â‚}$ and $ğ¯â‚‚ âˆˆ E_{Î»â‚‚}$ where $Î»â‚ â‰  Î»â‚‚$, then
\begin{align*}
Î»â‚âŸ¨ğ¯â‚, ğ¯â‚‚âŸ© &= âŸ¨T(ğ¯â‚), ğ¯â‚‚âŸ© \\
    &= âŸ¨ğ¯â‚, T(ğ¯â‚‚)âŸ© \\
    &= Î»â‚‚âŸ¨ğ¯â‚, ğ¯â‚‚âŸ©
\end{align*}
$$ âŸ¹ (Î»â‚ - Î»â‚‚)âŸ¨ğ¯â‚, ğ¯â‚‚âŸ© = 0 $$
but $Î»â‚ â‰  Î»â‚‚$ so $âŸ¨ğ¯â‚, ğ¯â‚‚âŸ© = 0$.

