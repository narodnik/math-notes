---
header-includes: |
    - \let\vec\mathbf
    - \let\M\parenMatrixstack
    - \let\;\quad
    - \def\nullsp{\mathcal{N}}
    - \def\qed{\hfill\blacksquare}
    - \DeclareMathOperator\sgn{sgn}
    - \DeclareMathOperator\dim{dim}
    - \DeclareMathOperator\row{row}
    - \DeclareMathOperator\col{col}
    - \DeclareMathOperator\span{span}
    - \DeclareMathOperator\ker{ker}
    - \DeclareMathOperator\im{im}
    - \DeclareMathOperator\rank{rank}
    - \DeclareMathOperator\det{det}
    - \DeclareMathOperator\Rot{Rot}
    - \DeclareMathOperator\charpoly{charpoly}
    - \DeclareMathOperator\Tr{Tr}
    - \DeclareMathOperator\Re{Re}
    - \DeclareMathOperator\diagonal{diagonal}
    - \DeclareMathOperator\diag{diag}
---

# $Ïƒáµ¢(A)$ Formula

First note that the determinant is multi-linear, so we can write
$$ A = \M{
    aâ‚â‚ - x, â‹¯  ;
    â‹¯, aâ‚™â‚™ - x
} = \M{
    aâ‚â‚, â‹¯  ;
    â‹¯, aâ‚™â‚™
} - \M{
    x, â‹¯    ;
    â‹¯, aâ‚™â‚™
} - \M{
    x, â‹¯    ;
    â‹¯, aâ‚™â‚™
} + \M{
    x, â‹¯    ;
    â‹¯, x
} $$

Assume we have a single $aáµ¢áµ¢ = x$. We are only interested in the coefficient for $x$.
Fixing $Ï€$ and using the Leibniz formula, we get
$$ k = -x âˆ‘_{Ï€ âˆˆ S(n) : Ï€(i)=i} \sgn(Ï€) a_{1Ï€(1)} â‹¯ a_{(i-1)Ï€(i-1)} a_{(i+1)Ï€(i+1)} â‹¯ a_{nÏ€(n)} $$
Using the same argument as for the Laplace expansion, when we have $P_Ï€$, and delete the $i$th column and row,
then $P_Ï€' = P_{Ï€'}$ corresponds to a valid $Ï€' âˆˆ S(n - 1)$. Then $\sgn(Ï€) = \sgn(Ï€')$, since the element
at $Ï€(i) = i$ is already in the identity position.
$$ k = -x \det(Aáµ¢áµ¢) $$
Using the multilinearity we see
$$ Ïƒâ‚(A) = -x âˆ‘áµ¢â‚Œâ‚â¿ \det(Aáµ¢áµ¢) $$

We can generalize the argument above, by fixing $Ï€(iâ‚) = iâ‚, â€¦, Ï€(iâ‚˜) = iâ‚˜$. Then we sum over the permutations
for the remaining minor matrices $Aâ‚â‚™â‚‹â‚˜â‚Žâ‚â‚™â‚‹â‚˜â‚Ž$. Note that in the multilinear expansion, the leading unit is $(-1)áµ$.

Thus we get the generalized formula for characteristic polynomial.

# Exercises 8.1

## Ex 8.1.10: Matrices with Same Charpoly are Similar

Let $B = \M{3, -2; 2, -1}$, then $\charpoly(B) = \charpoly(I)$ but they are not similar since $S = PIPâ»Â¹ = I$.

## Ex 8.1.12: $0$ is an Eigenvalue of $A âŸº A$ is Singular

This means the constant term of charpoly is $0$. Therefore there is a row $i$ with $(A - xI)áµ¢áµ¢ = -x$ and all other entries $0$.
Which means $A$ has a row of all zeroes, and hence is singular.

## Ex 8.1.15: $A$ and $-A$ are Similar

Prove $\det(A) = \Tr(A) = 0$. Note $A = -PAPâ»Â¹ âŸ¹  \det(A) = -\det(P)\det(A)\det(Pâ»Â¹) = -\det(A) âŸ¹  \det(A) = 0$. Then using $\Tr(AB) = \Tr(BA)$, we can do $\Tr(A) = \Tr(-PAPâ»Â¹) = -\Tr(-PPâ»Â¹A) = -\Tr(A) âŸ¹  \Tr(A) = 0$.

$\det(A) = Î»â‚Î»â‚‚Î»â‚ƒ = 0$ implies one of the $Î»áµ¢$ is $0$.
Assume WLOG that $Î»â‚ = 0$.

Since $\Tr(A) = Î»â‚ + Î»â‚‚ + Î»â‚ƒ = Î»â‚‚ + Î»â‚ƒ = 0 âŸ¹  Î»â‚‚ = -Î»â‚ƒ$. But since they determine the roots of charpoly with real coefficients $Î»â‚‚ = \bar{Î»â‚ƒ}$ also. This means $\Re(Î»â‚ƒ) = \Re(Î»â‚‚) = 0$. Thus the 2 remaining eigenvalues are complex with non-real part. This means both eigenvectors are also complex.

## Ex 8.1.19: Similar Matrices

```sage
sage: K = GF(2)
sage: A = matrix(K, [[1, 0, 1], [0, 1, 0], [1, 0, 1]])
sage: A.charpoly().factor()
(x + 1) * x^2
sage: A.determinant()
0
sage: A = matrix(K, [[1,0,0,0,1],[0,1,0,1,0],[0,0,1,0,0],[0,1,0,1,0],[1,0,0,0,1]])
sage: A.determinant()
0
sage: A.trace()
1
sage: A.charpoly().factor()
(x + 1) * x^4
```
Any diagonal matrix similar to $Xâ‚$ has the same charpoly.
Since it is diagonal, then $\diagonal(D - xI) = \{ x + 1, x, x \}$ (in any order)
$âŸ¹ \diagonal(D) = \{ 1, 0, 0 \}$. Since the trace and determinant are unchanged
for similar matrices, we see that $\det(D) â‰  \det(A)$ which is a contradiction.

## Ex 8.1.20: Complex Eigenvalues of Real Matrix are Conjugate Pairs

$$ p(x) âˆˆ â„[x] : p(z) = 0 = \repr{p(z)} = p(\repr{z}) \textrm{ since } p(x) = \repr{p(x)} $$

## Ex 8.1.22: Real Eigenvalues of Real Orthogonal Matrix are $Â±1$

Orthogonal matrix means $âŸ¨ð®áµ¢, ð®â±¼âŸ© = 0$ for all $i â‰  j$ or equivalently $Qáµ€Q = I$.
$$ Qð¯ = Î»ð¯ \; ð¯áµ€Qáµ€Qð¯ = Î»Â²âŸ¨ð¯, ð¯âŸ© = âŸ¨ð¯, ð¯âŸ© âŸ¹  Î» = Â±1 $$

# $A = â„³ ^â„¬_â„¬(T)$ has Eigenpair $(Î¼, ð±) âŸ¹ T$ has Eigenpair $(Î¼, â„¬ ð±)$

$$ ð¯ = âˆ‘ xáµ¢ ð¯áµ¢ = (ð¯â‚ â‹¯ ð¯â‚™) ð± $$
$$ (T(ð¯â‚) â‹¯ T(ð¯â‚™)) = (ð¯â‚ â‹¯ ð¯â‚™)A $$
Since $Að± = Î¼ð±$
\begin{align*}
T(ð¯) &= (T(ð¯â‚) â‹¯ T(ð¯â‚™)) ð± \\
    &= Î¼(ð¯â‚ â‹¯ ð¯â‚™)ð± \\
    &= Î¼ð¯
\end{align*}

# $A âˆˆ ð”½â¿$ has an Eigenbasis $ð°â‚, â€¦, ð°â‚™ âŸº A$ is Diagonalizable

$$ AP = (Að°â‚ â‹¯ Að°â‚™) = (ð°â‚ â‹¯ ð°â‚™) \diag(Î»â‚, â€¦, Î»â‚™) $$

# $T$ is Semisimple with Eigenbasis $(ð¯â‚ â‹¯ ð¯â‚™)P âŸº A$ is Diagonalizable

Let $â„¬ = \{ ð¯â‚, â€¦, ð¯â‚™ \}$ be a basis and $A = â„³ ^â„¬_â„¬ (T) = PDPâ»Â¹$. Since $T(ð¯â‚ â‹¯ ð¯â‚™) = (ð¯â‚ â‹¯ ð¯â‚™)A$ then
$$ (T(ð¯â‚) â‹¯ T(ð¯â‚™))P = (ð¯â‚ â‹¯ ð¯â‚™)PD $$
By 7.4.4, we have $TW = TVe = (TV)P$
$$ (T(ð°â‚) â‹¯ T(ð°â‚™)) = (T(ð¯â‚) â‹¯ T(ð¯â‚™)) â„³ ^â„¬_{â„¬ '} = (T(ð¯â‚) â‹¯ T(ð¯â‚™))P $$
but $A$ is defined by $T(ð¯â‚ â‹¯ ð¯â‚™) = (ð¯â‚ â‹¯ ð¯â‚™)A$ so
$$ T(ð°â‚ â‹¯ ð°â‚™) = (ð¯â‚ â‹¯ ð¯â‚™)AP = (ð°â‚ â‹¯ ð°â‚™)D $$

# Diagonalization via Eigenspace Decomposition

## Distinct Eigenvalues Produce Linearly Independent Eigenvectors

Let $Î±â‚ð¯â‚ + Î±â‚‚ð¯â‚‚ = ðŸŽ$. We want to show $Î±â‚ = Î±â‚‚ = 0$. Since $Að¯â‚ = Î»â‚$ and $Að¯â‚‚ = Î»â‚‚$,
so $A(Î±â‚ð¯â‚ + Î±â‚‚ð¯â‚‚) = Î±â‚Î»â‚ð¯â‚ + Î±â‚‚Î»â‚‚ð¯â‚‚ = ðŸŽ$. Subtracting $Î»â‚$ times the original equation,
we get
$$ Î±â‚‚(Î»â‚‚ - Î»â‚)ð¯â‚‚ = ðŸŽ âŸ¹ Î±â‚‚ = 0 $$
since $Î»â‚ â‰  Î»â‚‚$. Then we conclude $Î±â‚ = 0$.

## Union of Subset of Independent Eigenvectors is Independent

**Proposition 8.10:** *Suppose $Î»â‚, â€¦, Î»â‚˜ âˆˆ ð”½$ are distinct eigenvalues of $A âˆˆ ð”½^{nÃ—n}$, and choose
a set $Sáµ¢$ of linearly independent eigenvectors in the eigenspace
$$ E_{Î»áµ¢} = \{ ð¯ | (A - Î»áµ¢I)ð¯ = ðŸŽ \} = ð’© (A - Î»áµ¢I) $$
for all $1 â‰¤ i â‰¤ m$. Then the union of these linearly independent sets $S = Sâ‚ âˆª â‹¯ âˆª Sâ‚˜$ is also
linearly independent.*

Suppose $Aâ‚ = \{ \M{1, 0}áµ€ \}, Aâ‚‚ = \{ \M{0, 1}áµ€ \}, Aâ‚ƒ = \{ \M{1, 1}áµ€ \}$. Then $Aáµ¢ âˆ© Aâ±¼ = \{ ðŸŽ \}$
for all $i â‰  j$, but $Aâ‚ âˆª Aâ‚‚ âˆª Aâ‚ƒ$ is linearly dependent. So the result is not generally automatic
and needs to be proven for this specific case.

Denote the elements of $S$ as
$$ S = \{ ð®â‚â½Â¹â¾, â€¦, ð®â½Â¹â¾_{tâ‚}, ð®â‚â½Â²â¾, â€¦, ð®â½Â²â¾_{tâ‚‚}, â€¦, ð®â‚â½áµâ¾, â€¦, ð®â½áµâ¾_{tâ‚˜} \} $$
We want to show this set is linearly independent, so assume
$$ aâ‚â½Â¹â¾ð®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ð®â½Â¹â¾_{tâ‚} + aâ‚â½Â²â¾ð®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ð®â½Â²â¾_{tâ‚‚} + â‹¯ + aâ‚â½áµâ¾ ð®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ð®â½áµâ¾_{tâ‚˜} = ðŸŽ $$

\begin{align*}
Mâ‚ &= \{ ð®â‚â½Â¹â¾, â€¦, ð®â½Â¹â¾_{tâ‚} \} \\
Mâ‚‚ &= \{ ð®â‚â½Â²â¾, â€¦, ð®â½Â²â¾_{tâ‚‚} \} \\
    & â€¦ \\
Mâ‚˜ &= \{ ð®â‚â½áµâ¾, â€¦, ð®â½áµâ¾_{tâ‚˜} \}
\end{align*}

\begin{align*}
ð¯â‚ &= aâ‚â½Â¹â¾ ð®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ð®â½Â¹â¾_{tâ‚} âˆˆ E_{Î»â‚} \\
ð¯â‚‚ &= aâ‚â½Â²â¾ ð®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ð®â½Â²â¾_{tâ‚‚} âˆˆ E_{Î»â‚‚} \\
    & â€¦ \\
ð¯â‚˜ &= aâ‚â½áµâ¾ ð®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ð®â½áµâ¾_{tâ‚˜} âˆˆ E_{Î»â‚ƒ}
\end{align*}
The elements $\{ ð®â‚â½â±â¾, â€¦, ð®â½â±â¾_{táµ¢} \} âˆˆ E_{Î»áµ¢}$ were chosen to be linearly independent from the set $E_{Î»áµ¢}$ and we have
\begin{align*}
ðŸŽ &= aâ‚â½Â¹â¾ð®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ð®â½Â¹â¾_{tâ‚} + aâ‚â½Â²â¾ð®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ð®â½Â²â¾_{tâ‚‚} + â‹¯ + aâ‚â½áµâ¾ ð®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ð®â½áµâ¾_{tâ‚˜} \\
    &= (aâ‚â½Â¹â¾ð®â‚â½Â¹â¾ + â‹¯ + aâ½Â¹â¾_{tâ‚} ð®â½Â¹â¾_{tâ‚}) + (aâ‚â½Â²â¾ð®â‚â½Â²â¾ + â‹¯ + aâ½Â²â¾_{tâ‚‚} ð®â½Â²â¾_{tâ‚‚}) + â‹¯ + (aâ‚â½áµâ¾ ð®â‚â½áµâ¾ + â‹¯ + aâ½áµâ¾_{tâ‚˜} ð®â½áµâ¾_{tâ‚˜}) \\
    &= ð¯â‚ + ð¯â‚‚ + â‹¯ + ð¯â‚˜ \\
    &= âˆ‘áµ¢â‚Œâ‚áµ ð¯áµ¢
\end{align*}
Now we show all $ð¯áµ¢ = ðŸŽ$.

WLOG suppose $ð¯â‚ â‰  0$, then $ð¯â‚ = -(ð¯â‚‚ + â‹¯ + ð¯â‚˜) â‰  0$.

Now select from the set $\{ ð¯â‚‚, â€¦, ð¯â‚˜ \}$, a subset that form a basis of $W = \span\{ð¯â‚‚, â€¦, ð¯â‚˜\}$. Relabelling as necessary, denote these
by $\{ ð¯â‚‚, â€¦, ð¯â‚— \}$ where $l = \dim(W)$. Each of these $ð¯áµ¢ âˆˆ E_{Î»áµ¢}$ is an eigenvector with eigenvalue $Î»áµ¢$.

Since the eigenvalues are distinct $Î»â‚ â‰  Î»áµ¢$ for all $2 â‰¤ i â‰¤ l$. By the previous result that "distinct eigenvalues produce linearly
independent eigenvectors", this means that $bâ‚‚ = â‹¯ = bâ‚— = 0 âŸ¹ ð¯â‚ = ðŸŽ$.

Reapplying the same logic, we prove all $ð¯áµ¢ = ðŸŽ$. This means that
$$ ð¯áµ¢ = aâ‚â½â±â¾ ð®â‚â½â±â¾ + â‹¯ + aâ½â±â¾_{táµ¢} ð®â½â±â¾_{táµ¢} = ðŸŽ $$
Since each $ð®â±¼â½â±â¾$ is linearly independent within $\{ ð®â‚â½â±â¾, â€¦, ð®â½â±â¾_{táµ¢} \} âˆˆ E_{Î»áµ¢}$, this means $aâ‚â½â±â¾ = â‹¯ = aâ½â±â¾_{táµ¢} = 0$ for all $i$.

Therefore $S$ is linearly independent.

## Diagonalizable Condition $V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A)$

Let $Î»â‚, â€¦, Î»â‚˜$ denote the eigenvalues of $A$. $A$ is diagonalizable iff
$$ âˆ‘áµ¢â‚Œâ‚áµ \dim E_{Î»áµ¢}(A) = n $$
If $â„¬ áµ¢$ is a basis of $E_{Î»áµ¢}(A)$ then $â„¬ = â‹ƒ â„¬ áµ¢$ and
$$ V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A) $$

To prove this, use the previous proposition and the dimension theorem.

## $T$ is Semisimple $âŸ¹ (T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$

Since $T$ is semisimple, we have $V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A)$. So we just have to show that
for $ð¯áµ¢ âˆˆ E_{Î»áµ¢}(A)$ that
$$ (T - Î»â‚I)â‹¯(T - Î»â‚˜I)ð¯áµ¢ = ðŸŽ $$ {#eq:tl0}

Note that $(A - xI)(A - yI) = A - (x + y)I + xyI = (A - yI)(A - xI)$.
Therefore @eq:tl0 can be factored in the form $C(T - Î»áµ¢I)$
$$ âŸ¹ C(T - Î»áµ¢I)ð¯áµ¢ = ðŸŽ $$

## Alternative Proof of $T$ is Semisimple $âŸ¹ (T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$

\begin{align*}
f(A) &= aâ‚™Aâ¿ + â‹¯ + aâ‚A + aâ‚€I \\
    &= aâ‚™(PDPâ»Â¹)â¿ + â‹¯ + aâ‚(PDPâ»Â¹) + aâ‚€(PPâ»Â¹) \\
    &= aâ‚™PDâ¿Pâ»Â¹ + â‹¯ + aâ‚PDPâ»Â¹ + aâ‚€PPâ»Â¹ \\
    &= P(aâ‚™Dâ¿ + â‹¯ + aâ‚D + aâ‚€I)Pâ»Â¹ \\
    &= Pf(D)Pâ»Â¹
\end{align*}
Now let $g(X) = (X - Î»â‚I)â‹¯(X - Î»â‚˜I)$ then
$$ g(A) = Pg(D)Pâ»Â¹ $$
but $D = \diag(Î»â‚, â€¦, Î»â‚˜) âŸ¹ (D - Î»â‚)â‹¯(D - Î»â‚˜) = O$ so
$$ g(A) = O $$

## $T$ is Semisimple $âŸ¸ (T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$

### $Pâˆ˜Q = O$ and $\ker(P) âˆ© \ker(Q) = \{ ðŸŽ \} âŸ¹ V = \ker(P) âŠ• \ker(Q)$

2. $PQ = O$
3. $\ker(P) âˆ© \ker(Q) = \{ ðŸŽ \}$

$$ \dim(V) = \dim \ker(Q) + \dim \im(Q) $$
but $Pâˆ˜Q = O$ so $\im(Q) âŠ† \ker(P)$ and so
$$ \dim \im(Q) = \dim(V) - \dim \ker(Q) â‰¤ \dim \ker(P) $$
$$ âŸ¹ \dim(V) â‰¤ \dim \ker(Q) + \dim \ker(P) $$
Since $\ker(P) âˆ© \ker(Q) = \{ ðŸŽ \}$, we have $\dim(\ker(P) âˆ© \ker(Q)) = 0$. By the Grasmann intersection formula
\begin{align*}
\dim(\ker(P) + \ker(Q)) &= \dim \ker(P) + \dim \ker(Q) - \dim(\ker(P) âˆ© \ker(Q)) \\
    &= \dim \ker(P) + \dim \ker(Q)
\end{align*}
So we see that $\dim(V) â‰¤ \dim(\ker(P) + \ker(Q))$, but $\ker(P) + \ker(Q) âŠ† V$ so
$$ V = \ker(P) + \ker(Q) $$
Lastly since $\ker(P) âˆ© \ker(Q) = \{ ðŸŽ \}$, we have
$$ V = \ker(P) âŠ• \ker(Q) $$

### Generalizing Above to $V = \ker(Tâ‚) âŠ• â‹¯ âŠ• \ker(Tâ‚™)$

**Lemma.** *Suppose $Tâ‚, â€¦, Tâ‚™ : V â†’ V$ are linear mappings that satisfy the following properties:*

1. $Tâ‚âˆ˜â‹¯âˆ˜Tâ‚™ = O$
2. $Táµ¢âˆ˜Tâ±¼ = Tâ±¼âˆ˜Táµ¢$ for all $i, j$
3. $\ker(Táµ¢) âˆ© \ker(Táµ¢â‚Šâ‚âˆ˜â‹¯âˆ˜Tâ‚™) = \{ ðŸŽ \}$ for all $i$

*Then $V = \ker(Tâ‚) âŠ• â‹¯ âŠ• \ker(Tâ‚™)$.*

*Proof.* Let $P = Tâ‚$ and $Q = Tâ‚‚ âˆ˜ â‹¯ âˆ˜ Tâ‚™$, then
$$ V = \ker(Tâ‚) âŠ• \ker(Tâ‚‚ âˆ˜ â‹¯ âˆ˜ Tâ‚™) $$
Now set $V' = \ker(Tâ‚‚ âˆ˜ â‹¯ âˆ˜ Tâ‚™), P = Tâ‚‚, Q = Tâ‚ƒ âˆ˜ â‹¯ âˆ˜ Tâ‚™$ and we get
$$ V = \ker(Tâ‚) âŠ• \ker(Tâ‚‚) âŠ• \ker(Tâ‚ƒ âˆ˜ â‹¯ âˆ˜ Tâ‚™) $$
Iterating we arrive at the conclusion
$$ V = \ker(Tâ‚) âŠ• â‹¯ âŠ• \ker(Tâ‚™) $$

### $T$ is Semisimple

Condition (1) is our starting assumption $(T - Î»â‚I)â‹¯(T - Î»â‚˜I) = O$.

For (2), we can also see that $(T - Î»â‚I)(T - Î»â‚‚I) = (T - Î»â‚‚I)(T - Î»â‚I)$.

Finally to prove (3), let $ð± âˆˆ \ker(T - Î»áµ¢I)$ with $ð± â‰  0$, then $(T - Î»â±¼I)ð¯ = (Î»áµ¢ - Î»â±¼)ð¯$ and so
$$ (T - Î»áµ¢â‚Šâ‚I) â‹¯ (T - Î»â‚˜I)ð± = (Î»áµ¢ - Î»áµ¢â‚Šâ‚) â‹¯ (Î»áµ¢ - Î»â‚˜)ð± â‰  0 $$
since $Î»áµ¢$ are distinct from all $Î»â±¼$.

Then by the previous lemma
$$ V = E_{Î»â‚}(A) âŠ• â‹¯ âŠ• E_{Î»â‚˜}(A) $$

# Exercises 8.3

## Ex 8.3.6

Computing the eigenvalues gives 2 distinct real values.

## Ex 8.3.7

Let $Î»â‚, Î»â‚‚$ be the eigenvalues for $A$. Then
$$ \M{a, b; c, d}\M{xâ‚; yâ‚} = Î»â‚ \M{xâ‚; yâ‚} \; \M{a, b; c, d}\M{xâ‚‚; yâ‚‚} = Î»â‚‚ \M{xâ‚‚; yâ‚‚} $$
$$ axâ‚ + byâ‚ = Î»â‚xâ‚ $$
$$ axâ‚‚ + byâ‚‚ = Î»â‚‚xâ‚‚ $$
$(x, yÙ)$ is in the first quadrant means $\frac{x}{yÙ} > 0$. Likewise $(x, y)$ is in the second/fourth quadrant means $\frac{x}{y} < 0$.
Rewriting both equations, we get
$$ \frac{xâ‚}{yâ‚} = -\frac{b}{a - Î»â‚} $$
$$ \frac{xâ‚‚}{yâ‚‚} = -\frac{b}{a - Î»â‚‚} $$
Noting that $\Tr(A) = a + d = Î»â‚ + Î»â‚‚$ and $\det(A) = ad - bc = Î»â‚Î»â‚‚$ we see
$$ \frac{xâ‚xâ‚‚}{yâ‚yâ‚‚} = -\frac{b}{c} < 0 $$
So one fraction is in the first quadrant, and another in the second.

## Ex 8.3.10

$$ ð”½^{nÃ—n} = ð”½^{nÃ—n}â‚› âŠ• ð”½^{nÃ—n}â‚›â‚› $$
$$ A = \frac{1}{2}(A + Aáµ€) + \frac{1}{2}(A - Aáµ€) $$
When $A âˆˆ ð”½^{nÃ—n}â‚›$ then $ð•‹(A) = A$ so $Î»â‚ = 1$.

When $A âˆˆ ð”½^{nÃ—n}â‚›â‚›$ then $ð•‹(A) = -A$ so $Î»â‚ = -1$.

Now the basis for $ð”½^{nÃ—n}â‚›$ and $ð”½^{nÃ—n}â‚›â‚›$ are basis for $E_{Î»â‚}(ð•‹)$ and $E_{Î»â‚›}(ð•‹)$,
so $ð•‹$ is semisimple.

## Ex 8.3.15

$\charpoly(U) = (x - Î»â‚)â‹¯(x - Î»â‚™)$ where $Î»â‚, â€¦, Î»â‚™$ are the diagonals of $U$.
Each space $\dim E_{Î»áµ¢}(U) â‰¥ 1 âŸ¹ \dim E_{Î»â‚} = â‹¯ = \dim E_{Î»â‚™} = 1$ and so $âˆ‘ \dim E_{Î»áµ¢} = n âŸ¹ V = E_{Î»â‚} âŠ• â‹¯ âŠ• E_{Î»â‚™}$
and thus $U$ is diagonalizable.

## Ex 8.3.17/18

$A = PDâ‚Pâ»Â¹, B = PDâ‚‚Pâ»Â¹ âŸ¹ AB = PDâ‚Dâ‚‚Pâ»Â¹ = PDâ‚‚Dâ‚Pâ»Â¹ = BA$.

Let $ð¯ âˆˆ E_{Î»}(A)$, then $BAð¯ = Î»Bð¯ = ABð¯ âŸ¹ Bð¯ âˆˆ E_{Î»}(A)$.

For this question to work, we must have the additional condition that
$A$ has distinct eigenvalues. Let $\{ ð¯â‚, â€¦, ð¯â‚™ \}$ be an eigenbasis of $A$.
Then since $Bð¯áµ¢ âˆˆ E_{Î»áµ¢}(A)$ for all $i$, so $\{ Bð¯â‚, â€¦, Bð¯â‚™ \}$ is an eigenbasis
of $A$. Additionally since each $E_{Î»áµ¢}(A)$ forms a 1d subspace, it means that
$Bð¯áµ¢ = Î¼áµ¢ ð¯áµ¢$ for some $Î¼áµ¢$. Thus $\{ Bð¯â‚, â€¦, Bð¯â‚™ \}$ also forms an eigenbasis for $B$.

## Ex 8.3.19

TODO
